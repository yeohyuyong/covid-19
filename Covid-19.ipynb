{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid = pd.read_csv('covid_19_clean_complete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province/State</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Date</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>33.0000</td>\n",
       "      <td>65.0000</td>\n",
       "      <td>1/22/20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Albania</td>\n",
       "      <td>41.1533</td>\n",
       "      <td>20.1683</td>\n",
       "      <td>1/22/20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>28.0339</td>\n",
       "      <td>1.6596</td>\n",
       "      <td>1/22/20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>42.5063</td>\n",
       "      <td>1.5218</td>\n",
       "      <td>1/22/20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Angola</td>\n",
       "      <td>-11.2027</td>\n",
       "      <td>17.8739</td>\n",
       "      <td>1/22/20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Province/State Country/Region      Lat     Long     Date  Confirmed  Deaths  \\\n",
       "0            NaN    Afghanistan  33.0000  65.0000  1/22/20          0       0   \n",
       "1            NaN        Albania  41.1533  20.1683  1/22/20          0       0   \n",
       "2            NaN        Algeria  28.0339   1.6596  1/22/20          0       0   \n",
       "3            NaN        Andorra  42.5063   1.5218  1/22/20          0       0   \n",
       "4            NaN         Angola -11.2027  17.8739  1/22/20          0       0   \n",
       "\n",
       "   Recovered  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Province/State    16926\n",
       "Country/Region        0\n",
       "Lat                   0\n",
       "Long                  0\n",
       "Date                  0\n",
       "Confirmed             0\n",
       "Deaths                0\n",
       "Recovered             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24366 entries, 0 to 24365\n",
      "Data columns (total 8 columns):\n",
      "Province/State    7440 non-null object\n",
      "Country/Region    24366 non-null object\n",
      "Lat               24366 non-null float64\n",
      "Long              24366 non-null float64\n",
      "Date              24366 non-null object\n",
      "Confirmed         24366 non-null int64\n",
      "Deaths            24366 non-null int64\n",
      "Recovered         24366 non-null int64\n",
      "dtypes: float64(2), int64(3), object(3)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "covid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping unwanted colums\n",
    "covid.drop(['Province/State'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Date</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>33.0000</td>\n",
       "      <td>65.0000</td>\n",
       "      <td>1/22/20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>41.1533</td>\n",
       "      <td>20.1683</td>\n",
       "      <td>1/22/20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>28.0339</td>\n",
       "      <td>1.6596</td>\n",
       "      <td>1/22/20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>42.5063</td>\n",
       "      <td>1.5218</td>\n",
       "      <td>1/22/20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Angola</td>\n",
       "      <td>-11.2027</td>\n",
       "      <td>17.8739</td>\n",
       "      <td>1/22/20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Country/Region      Lat     Long     Date  Confirmed  Deaths  Recovered\n",
       "0    Afghanistan  33.0000  65.0000  1/22/20          0       0          0\n",
       "1        Albania  41.1533  20.1683  1/22/20          0       0          0\n",
       "2        Algeria  28.0339   1.6596  1/22/20          0       0          0\n",
       "3        Andorra  42.5063   1.5218  1/22/20          0       0          0\n",
       "4         Angola -11.2027  17.8739  1/22/20          0       0          0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid['Country/Region'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(covid['Country/Region'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid = pd.concat([covid.drop('Country/Region',axis=1),dummies],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid['Month'] = covid['Date'].apply(lambda date: date.split('/')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid.drop('Date',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = covid.drop('Deaths',axis=1).values\n",
    "y = covid['Deaths'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17056, 189)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(189,activation='relu'))\n",
    "model.add(Dense(95,activation='relu'))\n",
    "model.add(Dense(47,activation='relu'))\n",
    "model.add(Dense(24,activation='relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam',loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17056 samples, validate on 7310 samples\n",
      "Epoch 1/1000\n",
      "17056/17056 [==============================] - 1s 79us/sample - loss: 1620058.4319 - val_loss: 1024464.7565\n",
      "Epoch 2/1000\n",
      "17056/17056 [==============================] - 1s 47us/sample - loss: 739410.7018 - val_loss: 322342.8461\n",
      "Epoch 3/1000\n",
      "17056/17056 [==============================] - 1s 43us/sample - loss: 289604.0492 - val_loss: 243708.4187\n",
      "Epoch 4/1000\n",
      "17056/17056 [==============================] - 1s 50us/sample - loss: 234911.6484 - val_loss: 235425.4092\n",
      "Epoch 5/1000\n",
      "17056/17056 [==============================] - 1s 45us/sample - loss: 225124.3345 - val_loss: 225229.5981\n",
      "Epoch 6/1000\n",
      "17056/17056 [==============================] - 1s 44us/sample - loss: 220376.6285 - val_loss: 216286.6609\n",
      "Epoch 7/1000\n",
      "17056/17056 [==============================] - 1s 44us/sample - loss: 210851.6330 - val_loss: 228861.5298\n",
      "Epoch 8/1000\n",
      "17056/17056 [==============================] - 1s 45us/sample - loss: 205155.2076 - val_loss: 213446.5865\n",
      "Epoch 9/1000\n",
      "17056/17056 [==============================] - 1s 46us/sample - loss: 207172.6127 - val_loss: 217246.1218\n",
      "Epoch 10/1000\n",
      "17056/17056 [==============================] - 1s 46us/sample - loss: 206981.5328 - val_loss: 202302.0607\n",
      "Epoch 11/1000\n",
      "17056/17056 [==============================] - 1s 44us/sample - loss: 193883.8089 - val_loss: 206527.5977\n",
      "Epoch 12/1000\n",
      "17056/17056 [==============================] - 1s 46us/sample - loss: 199717.1555 - val_loss: 199559.9582\n",
      "Epoch 13/1000\n",
      "17056/17056 [==============================] - 1s 47us/sample - loss: 190779.1433 - val_loss: 190405.1060\n",
      "Epoch 14/1000\n",
      "17056/17056 [==============================] - 1s 45us/sample - loss: 185387.0489 - val_loss: 190425.6939\n",
      "Epoch 15/1000\n",
      "17056/17056 [==============================] - 1s 48us/sample - loss: 185231.0989 - val_loss: 183703.5910\n",
      "Epoch 16/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 178024.5911 - val_loss: 189139.4086\n",
      "Epoch 17/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 179460.6843 - val_loss: 175104.8612\n",
      "Epoch 18/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 174487.8701 - val_loss: 163031.6501\n",
      "Epoch 19/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 160329.4224 - val_loss: 158806.5148\n",
      "Epoch 20/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 156702.5709 - val_loss: 170374.4899\n",
      "Epoch 21/1000\n",
      "17056/17056 [==============================] - 1s 53us/sample - loss: 149231.1152 - val_loss: 142125.9179\n",
      "Epoch 22/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 139265.3965 - val_loss: 130029.5172\n",
      "Epoch 23/1000\n",
      "17056/17056 [==============================] - 1s 53us/sample - loss: 126290.3684 - val_loss: 118183.0507\n",
      "Epoch 24/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 119331.1350 - val_loss: 100554.8579\n",
      "Epoch 25/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 104698.5841 - val_loss: 87118.1800\n",
      "Epoch 26/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 87361.5774 - val_loss: 70053.2987\n",
      "Epoch 27/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 74584.9688 - val_loss: 60181.1300\n",
      "Epoch 28/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 58675.5589 - val_loss: 68745.3439\n",
      "Epoch 29/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 47789.7329 - val_loss: 51119.7957\n",
      "Epoch 30/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 38901.0381 - val_loss: 36955.7831\n",
      "Epoch 31/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 34014.5455 - val_loss: 29612.6270\n",
      "Epoch 32/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 32484.9527 - val_loss: 19249.6590\n",
      "Epoch 33/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 17103.4877 - val_loss: 18629.8878\n",
      "Epoch 34/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 13370.2256 - val_loss: 13089.9460\n",
      "Epoch 35/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 19210.5764 - val_loss: 12725.3556\n",
      "Epoch 36/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 11542.7577 - val_loss: 26476.1531\n",
      "Epoch 37/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 10176.3003 - val_loss: 7941.6734\n",
      "Epoch 38/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 9486.8453 - val_loss: 24635.5862\n",
      "Epoch 39/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 12255.7515 - val_loss: 5623.2235\n",
      "Epoch 40/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 10992.4163 - val_loss: 76772.5262\n",
      "Epoch 41/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 10491.7853 - val_loss: 7577.5659\n",
      "Epoch 42/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 8182.6384 - val_loss: 32597.9104\n",
      "Epoch 43/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 8018.4585 - val_loss: 8554.8037\n",
      "Epoch 44/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 8211.5552 - val_loss: 15160.9859\n",
      "Epoch 45/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 10333.2793 - val_loss: 10871.2570\n",
      "Epoch 46/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 10765.4955 - val_loss: 20967.9566\n",
      "Epoch 47/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 10247.1172 - val_loss: 15914.6561\n",
      "Epoch 48/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 6735.3957 - val_loss: 5893.4588\n",
      "Epoch 49/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 7376.2853 - val_loss: 13986.3831\n",
      "Epoch 50/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 9180.9836 - val_loss: 8986.7923\n",
      "Epoch 51/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 6880.9167 - val_loss: 4204.1671\n",
      "Epoch 52/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 11372.7453 - val_loss: 14854.8258\n",
      "Epoch 53/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 6854.3085 - val_loss: 9579.7793\n",
      "Epoch 54/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 10869.4575 - val_loss: 17449.7840\n",
      "Epoch 55/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 7856.7290 - val_loss: 11150.9116\n",
      "Epoch 56/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 6812.6601 - val_loss: 10175.2938\n",
      "Epoch 57/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 6549.3343 - val_loss: 8033.0405\n",
      "Epoch 58/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 6615.5781 - val_loss: 7798.1202\n",
      "Epoch 59/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 6684.1497 - val_loss: 8461.9651\n",
      "Epoch 60/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 9350.7132 - val_loss: 5073.2103\n",
      "Epoch 61/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 14370.3422 - val_loss: 22621.7817\n",
      "Epoch 62/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 6177.2494 - val_loss: 5499.9882\n",
      "Epoch 63/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 8934.3090 - val_loss: 6741.1530\n",
      "Epoch 64/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 6757.0371 - val_loss: 6666.3455\n",
      "Epoch 65/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 9608.1321 - val_loss: 6994.5731\n",
      "Epoch 66/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 9654.0359 - val_loss: 52983.2579\n",
      "Epoch 67/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 7807.5796 - val_loss: 8547.3318\n",
      "Epoch 68/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 8489.8568 - val_loss: 19592.2558\n",
      "Epoch 69/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 9517.3757 - val_loss: 7277.4564\n",
      "Epoch 70/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17056/17056 [==============================] - 1s 55us/sample - loss: 5465.0969 - val_loss: 3943.3230\n",
      "Epoch 71/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 6559.1021 - val_loss: 5324.6826\n",
      "Epoch 72/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 21035.5237 - val_loss: 7873.2218\n",
      "Epoch 73/1000\n",
      "17056/17056 [==============================] - 1s 66us/sample - loss: 4915.0495 - val_loss: 6266.0334\n",
      "Epoch 74/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 7052.3736 - val_loss: 5514.4972\n",
      "Epoch 75/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 6429.9531 - val_loss: 13716.5572\n",
      "Epoch 76/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 7265.0803 - val_loss: 9407.6634\n",
      "Epoch 77/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 8474.1462 - val_loss: 8731.4503\n",
      "Epoch 78/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 6306.2641 - val_loss: 4361.2838\n",
      "Epoch 79/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 7337.2795 - val_loss: 29132.9471\n",
      "Epoch 80/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 8258.0055 - val_loss: 11004.6985\n",
      "Epoch 81/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 6948.0587 - val_loss: 5922.2238\n",
      "Epoch 82/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 5902.0247 - val_loss: 24846.2766\n",
      "Epoch 83/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 8182.9756 - val_loss: 5046.2126\n",
      "Epoch 84/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 7141.6394 - val_loss: 4014.1116\n",
      "Epoch 85/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 5247.2740 - val_loss: 5110.0895\n",
      "Epoch 86/1000\n",
      "17056/17056 [==============================] - 1s 66us/sample - loss: 9154.6327 - val_loss: 14214.5255\n",
      "Epoch 87/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 6852.2411 - val_loss: 6226.3458\n",
      "Epoch 88/1000\n",
      "17056/17056 [==============================] - 1s 66us/sample - loss: 5630.9656 - val_loss: 3158.2478\n",
      "Epoch 89/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 8544.0197 - val_loss: 13146.1321\n",
      "Epoch 90/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 5826.5589 - val_loss: 11915.6107\n",
      "Epoch 91/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 4987.5352 - val_loss: 4433.3837\n",
      "Epoch 92/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 6883.8401 - val_loss: 9053.6294\n",
      "Epoch 93/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 5535.4167 - val_loss: 4829.6462\n",
      "Epoch 94/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 6401.4202 - val_loss: 10945.3595\n",
      "Epoch 95/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 9844.9887 - val_loss: 3139.3776\n",
      "Epoch 96/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 5710.1585 - val_loss: 4455.4972\n",
      "Epoch 97/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 4105.8325 - val_loss: 3870.7143\n",
      "Epoch 98/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 11950.7906 - val_loss: 27790.1344\n",
      "Epoch 99/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 5589.9438 - val_loss: 4720.7153\n",
      "Epoch 100/1000\n",
      "17056/17056 [==============================] - 1s 53us/sample - loss: 4773.6689 - val_loss: 7202.1466\n",
      "Epoch 101/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 5513.6471 - val_loss: 3710.1126\n",
      "Epoch 102/1000\n",
      "17056/17056 [==============================] - 1s 66us/sample - loss: 11655.8935 - val_loss: 3132.8763\n",
      "Epoch 103/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 5657.6433 - val_loss: 3699.5056\n",
      "Epoch 104/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 8140.4216 - val_loss: 8748.5529\n",
      "Epoch 105/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 7463.2277 - val_loss: 5033.7994\n",
      "Epoch 106/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 6411.2428 - val_loss: 5187.2313\n",
      "Epoch 107/1000\n",
      "17056/17056 [==============================] - 1s 81us/sample - loss: 4248.5600 - val_loss: 3370.3824\n",
      "Epoch 108/1000\n",
      "17056/17056 [==============================] - 2s 90us/sample - loss: 6317.6648 - val_loss: 17517.9988\n",
      "Epoch 109/1000\n",
      "17056/17056 [==============================] - 1s 69us/sample - loss: 4964.9424 - val_loss: 10974.7191\n",
      "Epoch 110/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 4992.0804 - val_loss: 4472.9998\n",
      "Epoch 111/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 5616.1137 - val_loss: 10180.6245\n",
      "Epoch 112/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 5006.4124 - val_loss: 3715.2950\n",
      "Epoch 113/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 10789.2622 - val_loss: 3221.3587\n",
      "Epoch 114/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 4897.8373 - val_loss: 4422.3024\n",
      "Epoch 115/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 5152.7141 - val_loss: 24469.2662\n",
      "Epoch 116/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 7493.4230 - val_loss: 4291.6901\n",
      "Epoch 117/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 4115.3248 - val_loss: 6938.6226\n",
      "Epoch 118/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 3607.7412 - val_loss: 3108.5119\n",
      "Epoch 119/1000\n",
      "17056/17056 [==============================] - 1s 76us/sample - loss: 4561.4838 - val_loss: 8920.1276\n",
      "Epoch 120/1000\n",
      "17056/17056 [==============================] - 1s 48us/sample - loss: 10004.4834 - val_loss: 10919.7429\n",
      "Epoch 121/1000\n",
      "17056/17056 [==============================] - 1s 49us/sample - loss: 3519.8661 - val_loss: 3775.0871\n",
      "Epoch 122/1000\n",
      "17056/17056 [==============================] - 1s 49us/sample - loss: 4798.0075 - val_loss: 8642.4151\n",
      "Epoch 123/1000\n",
      "17056/17056 [==============================] - 1s 52us/sample - loss: 5803.6698 - val_loss: 5103.6261\n",
      "Epoch 124/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 8659.8663 - val_loss: 7613.9796\n",
      "Epoch 125/1000\n",
      "17056/17056 [==============================] - 1s 49us/sample - loss: 7595.8718 - val_loss: 6868.3547\n",
      "Epoch 126/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 4765.6203 - val_loss: 5434.3947\n",
      "Epoch 127/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 4640.2762 - val_loss: 9132.4684\n",
      "Epoch 128/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 6530.1330 - val_loss: 16146.5102\n",
      "Epoch 129/1000\n",
      "17056/17056 [==============================] - 1s 48us/sample - loss: 9708.3588 - val_loss: 5722.0629\n",
      "Epoch 130/1000\n",
      "17056/17056 [==============================] - 1s 47us/sample - loss: 5745.9618 - val_loss: 5893.9463\n",
      "Epoch 131/1000\n",
      "17056/17056 [==============================] - 1s 48us/sample - loss: 3637.1227 - val_loss: 4123.7800\n",
      "Epoch 132/1000\n",
      "17056/17056 [==============================] - 1s 50us/sample - loss: 6599.4491 - val_loss: 5052.0210\n",
      "Epoch 133/1000\n",
      "17056/17056 [==============================] - 1s 49us/sample - loss: 5363.6551 - val_loss: 3191.8820\n",
      "Epoch 134/1000\n",
      "17056/17056 [==============================] - 1s 51us/sample - loss: 5270.9274 - val_loss: 3685.0672\n",
      "Epoch 135/1000\n",
      "17056/17056 [==============================] - 1s 53us/sample - loss: 5027.5763 - val_loss: 26643.4048\n",
      "Epoch 136/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 5226.3085 - val_loss: 3874.8115\n",
      "Epoch 137/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 6894.2933 - val_loss: 3804.4963\n",
      "Epoch 138/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 3623.9363 - val_loss: 2179.1259\n",
      "Epoch 139/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 4160.7533 - val_loss: 5148.8310\n",
      "Epoch 140/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17056/17056 [==============================] - 1s 59us/sample - loss: 8608.1059 - val_loss: 3019.1671\n",
      "Epoch 141/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 4841.5773 - val_loss: 4215.4266\n",
      "Epoch 142/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 4108.7953 - val_loss: 9227.3203\n",
      "Epoch 143/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 5265.0687 - val_loss: 2567.4973\n",
      "Epoch 144/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 8423.6211 - val_loss: 2494.3896\n",
      "Epoch 145/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 4281.8299 - val_loss: 9957.8888\n",
      "Epoch 146/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 6137.1698 - val_loss: 4676.3536\n",
      "Epoch 147/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 5219.9807 - val_loss: 44214.5081\n",
      "Epoch 148/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 7739.9443 - val_loss: 8028.9069\n",
      "Epoch 149/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 3668.4631 - val_loss: 9458.8866\n",
      "Epoch 150/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 5687.2346 - val_loss: 2184.5899\n",
      "Epoch 151/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 3496.2979 - val_loss: 10437.9631\n",
      "Epoch 152/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 9528.3184 - val_loss: 4472.7307\n",
      "Epoch 153/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 3207.9520 - val_loss: 3069.2239\n",
      "Epoch 154/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 5972.2696 - val_loss: 3377.6715\n",
      "Epoch 155/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 5220.7322 - val_loss: 2919.3137\n",
      "Epoch 156/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 4249.6374 - val_loss: 4556.4037\n",
      "Epoch 157/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 4851.4059 - val_loss: 10035.6079\n",
      "Epoch 158/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 7637.7947 - val_loss: 3195.4306\n",
      "Epoch 159/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 3475.1743 - val_loss: 7355.8674\n",
      "Epoch 160/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 4660.7874 - val_loss: 3536.0100\n",
      "Epoch 161/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 4097.0797 - val_loss: 3305.4906\n",
      "Epoch 162/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 4417.8964 - val_loss: 3855.2833\n",
      "Epoch 163/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 4766.2486 - val_loss: 3627.5946\n",
      "Epoch 164/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 4167.2783 - val_loss: 3234.9804\n",
      "Epoch 165/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 10782.4479 - val_loss: 4458.2841\n",
      "Epoch 166/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 3124.8085 - val_loss: 5637.4790\n",
      "Epoch 167/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 3567.6808 - val_loss: 3120.6094\n",
      "Epoch 168/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 4257.6418 - val_loss: 2027.9631\n",
      "Epoch 169/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 7764.7242 - val_loss: 5844.4019\n",
      "Epoch 170/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 6693.3265 - val_loss: 13471.2321\n",
      "Epoch 171/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 3442.5607 - val_loss: 3966.1737\n",
      "Epoch 172/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 6459.3283 - val_loss: 3801.8755\n",
      "Epoch 173/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 4461.8553 - val_loss: 6128.1673\n",
      "Epoch 174/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 5547.8438 - val_loss: 3702.2051\n",
      "Epoch 175/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 5234.3861 - val_loss: 4342.9935\n",
      "Epoch 176/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 3634.8261 - val_loss: 4629.7929\n",
      "Epoch 177/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 4878.9848 - val_loss: 2917.3812\n",
      "Epoch 178/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 4067.3931 - val_loss: 6337.5658\n",
      "Epoch 179/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 7149.5001 - val_loss: 5126.0537\n",
      "Epoch 180/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 3175.4726 - val_loss: 3355.6139\n",
      "Epoch 181/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 7166.0300 - val_loss: 5109.2131\n",
      "Epoch 182/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 5227.9243 - val_loss: 2582.4950\n",
      "Epoch 183/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 3795.5672 - val_loss: 2188.9494\n",
      "Epoch 184/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 3656.1382 - val_loss: 5375.1279\n",
      "Epoch 185/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 5899.9120 - val_loss: 7389.0202\n",
      "Epoch 186/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 4515.4607 - val_loss: 15915.1284\n",
      "Epoch 187/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 7162.3513 - val_loss: 8580.6994\n",
      "Epoch 188/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 5207.0262 - val_loss: 4439.0564\n",
      "Epoch 189/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 3975.5905 - val_loss: 6004.6676\n",
      "Epoch 190/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 3627.7890 - val_loss: 2908.6104\n",
      "Epoch 191/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 3575.6346 - val_loss: 6229.2160\n",
      "Epoch 192/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 6289.7720 - val_loss: 8217.2201\n",
      "Epoch 193/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 3406.5847 - val_loss: 2740.7917\n",
      "Epoch 194/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 3400.2873 - val_loss: 3097.0817\n",
      "Epoch 195/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 3247.1019 - val_loss: 3652.5752\n",
      "Epoch 196/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 5921.3677 - val_loss: 4747.7205\n",
      "Epoch 197/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 4718.2470 - val_loss: 4586.5780\n",
      "Epoch 198/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 6898.5712 - val_loss: 3222.4147\n",
      "Epoch 199/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 3479.6587 - val_loss: 2626.4018\n",
      "Epoch 200/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 3223.4399 - val_loss: 6075.4873\n",
      "Epoch 201/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 3495.7317 - val_loss: 6889.4436\n",
      "Epoch 202/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 7418.9397 - val_loss: 9750.3974\n",
      "Epoch 203/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 4468.2559 - val_loss: 15473.3388\n",
      "Epoch 204/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 3905.2653 - val_loss: 7688.7414\n",
      "Epoch 205/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 5902.8429 - val_loss: 4933.8255\n",
      "Epoch 206/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 3748.2508 - val_loss: 2623.8970\n",
      "Epoch 207/1000\n",
      "17056/17056 [==============================] - 1s 68us/sample - loss: 4068.7820 - val_loss: 5238.2345\n",
      "Epoch 208/1000\n",
      "17056/17056 [==============================] - 1s 65us/sample - loss: 4066.1764 - val_loss: 5704.9966\n",
      "Epoch 209/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 4123.9940 - val_loss: 9259.5110\n",
      "Epoch 210/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17056/17056 [==============================] - 1s 64us/sample - loss: 4410.8824 - val_loss: 3022.5795\n",
      "Epoch 211/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 2584.6542 - val_loss: 3923.7227\n",
      "Epoch 212/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 3666.5798 - val_loss: 5662.1023\n",
      "Epoch 213/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 5999.1708 - val_loss: 5022.8366\n",
      "Epoch 214/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 4521.1579 - val_loss: 4050.0568\n",
      "Epoch 215/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 3805.0706 - val_loss: 2001.3310\n",
      "Epoch 216/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 3160.6717 - val_loss: 7544.4990\n",
      "Epoch 217/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 4646.5940 - val_loss: 2278.2079\n",
      "Epoch 218/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 3389.5640 - val_loss: 2295.6050\n",
      "Epoch 219/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 4254.4927 - val_loss: 2862.9623\n",
      "Epoch 220/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 6983.1017 - val_loss: 5734.1424\n",
      "Epoch 221/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 6825.9182 - val_loss: 2087.8115\n",
      "Epoch 222/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 2497.9399 - val_loss: 2665.1381\n",
      "Epoch 223/1000\n",
      "17056/17056 [==============================] - 1s 65us/sample - loss: 4364.6779 - val_loss: 2341.8068\n",
      "Epoch 224/1000\n",
      "17056/17056 [==============================] - 1s 69us/sample - loss: 4312.2322 - val_loss: 3305.7288\n",
      "Epoch 225/1000\n",
      "17056/17056 [==============================] - 1s 74us/sample - loss: 3561.9197 - val_loss: 1877.2538\n",
      "Epoch 226/1000\n",
      "17056/17056 [==============================] - 1s 67us/sample - loss: 2936.6384 - val_loss: 6851.2834\n",
      "Epoch 227/1000\n",
      "17056/17056 [==============================] - 1s 68us/sample - loss: 3229.0110 - val_loss: 3429.3892\n",
      "Epoch 228/1000\n",
      "17056/17056 [==============================] - 1s 66us/sample - loss: 5047.6416 - val_loss: 3659.3055\n",
      "Epoch 229/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 3950.7821 - val_loss: 2984.5598\n",
      "Epoch 230/1000\n",
      "17056/17056 [==============================] - 1s 68us/sample - loss: 3805.1624 - val_loss: 8718.0258\n",
      "Epoch 231/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 6031.9469 - val_loss: 11527.8420\n",
      "Epoch 232/1000\n",
      "17056/17056 [==============================] - 1s 67us/sample - loss: 4841.5211 - val_loss: 3730.0575\n",
      "Epoch 233/1000\n",
      "17056/17056 [==============================] - 1s 72us/sample - loss: 2996.7611 - val_loss: 4833.5308\n",
      "Epoch 234/1000\n",
      "17056/17056 [==============================] - 1s 70us/sample - loss: 5814.0199 - val_loss: 8542.9524\n",
      "Epoch 235/1000\n",
      "17056/17056 [==============================] - 1s 65us/sample - loss: 4147.5526 - val_loss: 6160.8749\n",
      "Epoch 236/1000\n",
      "17056/17056 [==============================] - 1s 66us/sample - loss: 2653.6055 - val_loss: 1991.1629\n",
      "Epoch 237/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 5552.4785 - val_loss: 5117.5755\n",
      "Epoch 238/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 3124.8636 - val_loss: 3386.6725\n",
      "Epoch 239/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 3174.2633 - val_loss: 3167.2753\n",
      "Epoch 240/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 4163.0074 - val_loss: 6477.5820\n",
      "Epoch 241/1000\n",
      "17056/17056 [==============================] - 1s 66us/sample - loss: 5160.7086 - val_loss: 3639.8704\n",
      "Epoch 242/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 3036.5770 - val_loss: 4848.6744\n",
      "Epoch 243/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 4749.6062 - val_loss: 3060.8992\n",
      "Epoch 244/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 2566.4801 - val_loss: 3033.9376\n",
      "Epoch 245/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 4271.0390 - val_loss: 25717.6616\n",
      "Epoch 246/1000\n",
      "17056/17056 [==============================] - 1s 65us/sample - loss: 3692.5989 - val_loss: 11185.8938\n",
      "Epoch 247/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 5671.6187 - val_loss: 2870.7996\n",
      "Epoch 248/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 2882.8225 - val_loss: 4644.7020\n",
      "Epoch 249/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 6534.9085 - val_loss: 2789.0845\n",
      "Epoch 250/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 3268.4517 - val_loss: 2402.3919\n",
      "Epoch 251/1000\n",
      "17056/17056 [==============================] - 1s 53us/sample - loss: 2946.1699 - val_loss: 1966.8751\n",
      "Epoch 252/1000\n",
      "17056/17056 [==============================] - 1s 52us/sample - loss: 3937.2924 - val_loss: 11787.6206\n",
      "Epoch 253/1000\n",
      "17056/17056 [==============================] - 1s 51us/sample - loss: 4134.1647 - val_loss: 3169.9632\n",
      "Epoch 254/1000\n",
      "17056/17056 [==============================] - 1s 51us/sample - loss: 5866.4597 - val_loss: 4108.5895\n",
      "Epoch 255/1000\n",
      "17056/17056 [==============================] - 1s 51us/sample - loss: 2906.2298 - val_loss: 12333.9710\n",
      "Epoch 256/1000\n",
      "17056/17056 [==============================] - 1s 53us/sample - loss: 4468.7898 - val_loss: 16491.6202\n",
      "Epoch 257/1000\n",
      "17056/17056 [==============================] - 1s 51us/sample - loss: 2925.8489 - val_loss: 2565.9537\n",
      "Epoch 258/1000\n",
      "17056/17056 [==============================] - 1s 52us/sample - loss: 5878.0732 - val_loss: 6420.7161\n",
      "Epoch 259/1000\n",
      "17056/17056 [==============================] - 1s 53us/sample - loss: 3312.4552 - val_loss: 4739.5961\n",
      "Epoch 260/1000\n",
      "17056/17056 [==============================] - 1s 52us/sample - loss: 5154.1310 - val_loss: 5399.8625\n",
      "Epoch 261/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 4206.7357 - val_loss: 2815.2662\n",
      "Epoch 262/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 4675.1192 - val_loss: 3944.1630\n",
      "Epoch 263/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 4356.3254 - val_loss: 7581.3061\n",
      "Epoch 264/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 3823.5992 - val_loss: 3561.0533\n",
      "Epoch 265/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 3127.1330 - val_loss: 24735.6949\n",
      "Epoch 266/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 6386.9496 - val_loss: 3441.7433\n",
      "Epoch 267/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 4675.7789 - val_loss: 31030.7232\n",
      "Epoch 268/1000\n",
      "17056/17056 [==============================] - 1s 68us/sample - loss: 3351.7062 - val_loss: 2484.1164\n",
      "Epoch 269/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 4706.1506 - val_loss: 5125.2835\n",
      "Epoch 270/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 2900.9413 - val_loss: 8769.7657\n",
      "Epoch 271/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 3531.2236 - val_loss: 8866.8270\n",
      "Epoch 272/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 3619.8845 - val_loss: 3923.7260\n",
      "Epoch 273/1000\n",
      "17056/17056 [==============================] - 1s 68us/sample - loss: 5495.0833 - val_loss: 3129.3258\n",
      "Epoch 274/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 3128.0281 - val_loss: 4711.4690\n",
      "Epoch 275/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 3693.6524 - val_loss: 4685.3831\n",
      "Epoch 276/1000\n",
      "17056/17056 [==============================] - 1s 53us/sample - loss: 2741.9014 - val_loss: 6347.7829\n",
      "Epoch 277/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 3985.7649 - val_loss: 3431.8237\n",
      "Epoch 278/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 6207.1703 - val_loss: 3714.7428\n",
      "Epoch 279/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 4399.9649 - val_loss: 16309.8010\n",
      "Epoch 280/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17056/17056 [==============================] - 1s 55us/sample - loss: 2930.9688 - val_loss: 2372.7103\n",
      "Epoch 281/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 2872.0742 - val_loss: 2869.4004\n",
      "Epoch 282/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 3167.8374 - val_loss: 6250.6806\n",
      "Epoch 283/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 3967.5388 - val_loss: 4316.3151\n",
      "Epoch 284/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 2645.3048 - val_loss: 5679.4825\n",
      "Epoch 285/1000\n",
      "17056/17056 [==============================] - 1s 52us/sample - loss: 5088.0941 - val_loss: 3724.2357\n",
      "Epoch 286/1000\n",
      "17056/17056 [==============================] - 1s 53us/sample - loss: 2716.0821 - val_loss: 2602.3204\n",
      "Epoch 287/1000\n",
      "17056/17056 [==============================] - 1s 53us/sample - loss: 5194.7712 - val_loss: 2202.5790\n",
      "Epoch 288/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 3153.3084 - val_loss: 3934.7124\n",
      "Epoch 289/1000\n",
      "17056/17056 [==============================] - 1s 53us/sample - loss: 2940.6795 - val_loss: 4144.6426\n",
      "Epoch 290/1000\n",
      "17056/17056 [==============================] - 1s 53us/sample - loss: 2530.8940 - val_loss: 4046.8121\n",
      "Epoch 291/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 3227.8031 - val_loss: 4728.3335\n",
      "Epoch 292/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 4527.9051 - val_loss: 4529.6711\n",
      "Epoch 293/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 2788.5205 - val_loss: 2615.6682\n",
      "Epoch 294/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 5074.4539 - val_loss: 4616.6197\n",
      "Epoch 295/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 2995.3093 - val_loss: 3298.4408\n",
      "Epoch 296/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 2419.3698 - val_loss: 7420.2574\n",
      "Epoch 297/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 3603.8296 - val_loss: 2612.6211\n",
      "Epoch 298/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 3763.2408 - val_loss: 2599.8130\n",
      "Epoch 299/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 2813.5399 - val_loss: 2219.6080\n",
      "Epoch 300/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 3648.0224 - val_loss: 2526.4308\n",
      "Epoch 301/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 4898.4962 - val_loss: 2876.5422\n",
      "Epoch 302/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 2633.1650 - val_loss: 2963.5999\n",
      "Epoch 303/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 2543.3287 - val_loss: 3481.6681\n",
      "Epoch 304/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 2722.0432 - val_loss: 3280.4278\n",
      "Epoch 305/1000\n",
      "17056/17056 [==============================] - 1s 67us/sample - loss: 5253.2843 - val_loss: 4717.2651\n",
      "Epoch 306/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 3545.7046 - val_loss: 1952.0005\n",
      "Epoch 307/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 2577.2874 - val_loss: 10733.9356\n",
      "Epoch 308/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 5318.4308 - val_loss: 2683.8114\n",
      "Epoch 309/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 3100.0606 - val_loss: 2054.4695\n",
      "Epoch 310/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 3199.9003 - val_loss: 2351.3937\n",
      "Epoch 311/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 4287.4347 - val_loss: 2812.2639\n",
      "Epoch 312/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 2273.5974 - val_loss: 5518.6934\n",
      "Epoch 313/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 3027.4362 - val_loss: 7465.9863\n",
      "Epoch 314/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 2725.9892 - val_loss: 4046.8623\n",
      "Epoch 315/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 2856.5505 - val_loss: 8405.0874\n",
      "Epoch 316/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 5810.8473 - val_loss: 5258.7301\n",
      "Epoch 317/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 7356.5933 - val_loss: 6445.9587\n",
      "Epoch 318/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 2100.3835 - val_loss: 5686.0888\n",
      "Epoch 319/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 2494.0095 - val_loss: 5718.8229\n",
      "Epoch 320/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 4627.4824 - val_loss: 8361.7363\n",
      "Epoch 321/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 3503.0850 - val_loss: 4080.0984\n",
      "Epoch 322/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 2964.6151 - val_loss: 2945.8522\n",
      "Epoch 323/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 4040.0499 - val_loss: 2239.1510\n",
      "Epoch 324/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 4576.7262 - val_loss: 8709.8578\n",
      "Epoch 325/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 2917.6921 - val_loss: 2434.7423\n",
      "Epoch 326/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 2094.6332 - val_loss: 22872.9155\n",
      "Epoch 327/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 6160.7387 - val_loss: 2713.5470\n",
      "Epoch 328/1000\n",
      "17056/17056 [==============================] - 1s 87us/sample - loss: 2282.5944 - val_loss: 4411.5095\n",
      "Epoch 329/1000\n",
      "17056/17056 [==============================] - 1s 75us/sample - loss: 3219.6899 - val_loss: 3370.3665\n",
      "Epoch 330/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 2664.7555 - val_loss: 6819.5597\n",
      "Epoch 331/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 3858.2463 - val_loss: 4357.3157\n",
      "Epoch 332/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 2073.4878 - val_loss: 5253.4378\n",
      "Epoch 333/1000\n",
      "17056/17056 [==============================] - 1s 66us/sample - loss: 3320.2541 - val_loss: 4796.4928\n",
      "Epoch 334/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 3609.5436 - val_loss: 3267.8085\n",
      "Epoch 335/1000\n",
      "17056/17056 [==============================] - 1s 66us/sample - loss: 3702.1663 - val_loss: 9149.9211\n",
      "Epoch 336/1000\n",
      "17056/17056 [==============================] - 1s 86us/sample - loss: 4036.9589 - val_loss: 2377.7686\n",
      "Epoch 337/1000\n",
      "17056/17056 [==============================] - 1s 70us/sample - loss: 7635.6597 - val_loss: 6995.6558\n",
      "Epoch 338/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 2526.1271 - val_loss: 1945.4695\n",
      "Epoch 339/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 2491.0078 - val_loss: 5610.2282\n",
      "Epoch 340/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 2745.5340 - val_loss: 6782.8701\n",
      "Epoch 341/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 3228.5104 - val_loss: 5155.1847\n",
      "Epoch 342/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 2636.7914 - val_loss: 6289.0227\n",
      "Epoch 343/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 3018.6659 - val_loss: 5927.1265\n",
      "Epoch 344/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 4174.3317 - val_loss: 2865.8913\n",
      "Epoch 345/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 3484.1416 - val_loss: 11396.3367\n",
      "Epoch 346/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 3192.5178 - val_loss: 1917.6847\n",
      "Epoch 347/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 2452.9717 - val_loss: 4535.5028\n",
      "Epoch 348/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 3325.7776 - val_loss: 7541.3179\n",
      "Epoch 349/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 4240.0878 - val_loss: 4443.0751\n",
      "Epoch 350/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17056/17056 [==============================] - 1s 61us/sample - loss: 3444.1243 - val_loss: 1985.7765\n",
      "Epoch 351/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 2568.6886 - val_loss: 5433.5507\n",
      "Epoch 352/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 4098.2187 - val_loss: 3513.0706\n",
      "Epoch 353/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 2592.9859 - val_loss: 2599.3398\n",
      "Epoch 354/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 2876.1021 - val_loss: 3379.2758\n",
      "Epoch 355/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 2984.4821 - val_loss: 5109.5503\n",
      "Epoch 356/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 3047.3311 - val_loss: 3690.5260\n",
      "Epoch 357/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 3259.8466 - val_loss: 2819.6569\n",
      "Epoch 358/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 2838.3702 - val_loss: 2664.7365\n",
      "Epoch 359/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 3783.1432 - val_loss: 7153.3775\n",
      "Epoch 360/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 3045.9406 - val_loss: 2109.8868\n",
      "Epoch 361/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 2834.7510 - val_loss: 3675.5656\n",
      "Epoch 362/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 2993.3469 - val_loss: 3208.5911\n",
      "Epoch 363/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 3871.7989 - val_loss: 5931.6101\n",
      "Epoch 364/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 2392.2279 - val_loss: 5999.0560\n",
      "Epoch 365/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 4486.3095 - val_loss: 3297.3797\n",
      "Epoch 366/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 2530.6243 - val_loss: 2461.9924\n",
      "Epoch 367/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 2535.3217 - val_loss: 4808.9773\n",
      "Epoch 368/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 4437.5401 - val_loss: 12823.6402\n",
      "Epoch 369/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 3537.4405 - val_loss: 2195.8843\n",
      "Epoch 370/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 2199.7853 - val_loss: 2557.1093\n",
      "Epoch 371/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 3299.8878 - val_loss: 9504.6506\n",
      "Epoch 372/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 2330.1401 - val_loss: 4020.4987\n",
      "Epoch 373/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 2664.4155 - val_loss: 12908.7047\n",
      "Epoch 374/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 3437.8115 - val_loss: 2499.2831\n",
      "Epoch 375/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 3790.6204 - val_loss: 6455.3873\n",
      "Epoch 376/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 3029.8860 - val_loss: 3965.4457\n",
      "Epoch 377/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 2722.9410 - val_loss: 2743.4581\n",
      "Epoch 378/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 2456.9503 - val_loss: 4087.3465\n",
      "Epoch 379/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 2641.5352 - val_loss: 4241.8170\n",
      "Epoch 380/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 2508.6439 - val_loss: 4435.8701\n",
      "Epoch 381/1000\n",
      "17056/17056 [==============================] - 1s 65us/sample - loss: 2619.5151 - val_loss: 11471.5909\n",
      "Epoch 382/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 3146.6500 - val_loss: 4727.9142\n",
      "Epoch 383/1000\n",
      "17056/17056 [==============================] - 1s 72us/sample - loss: 3893.7917 - val_loss: 3885.5213\n",
      "Epoch 384/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 2372.9624 - val_loss: 2241.1440\n",
      "Epoch 385/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 2556.3109 - val_loss: 3424.2561\n",
      "Epoch 386/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 3057.2651 - val_loss: 3202.3481\n",
      "Epoch 387/1000\n",
      "17056/17056 [==============================] - 1s 66us/sample - loss: 2289.9645 - val_loss: 4053.7016\n",
      "Epoch 388/1000\n",
      "17056/17056 [==============================] - 1s 65us/sample - loss: 5950.6431 - val_loss: 2292.4677\n",
      "Epoch 389/1000\n",
      "17056/17056 [==============================] - 1s 67us/sample - loss: 2381.0703 - val_loss: 4502.0946\n",
      "Epoch 390/1000\n",
      "17056/17056 [==============================] - 1s 66us/sample - loss: 2658.2077 - val_loss: 4680.5254\n",
      "Epoch 391/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 3925.5384 - val_loss: 2794.5937\n",
      "Epoch 392/1000\n",
      "17056/17056 [==============================] - 1s 65us/sample - loss: 2453.4905 - val_loss: 9168.2021\n",
      "Epoch 393/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 3647.5204 - val_loss: 2157.4634\n",
      "Epoch 394/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 2743.7072 - val_loss: 7333.5785\n",
      "Epoch 395/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 2811.1337 - val_loss: 5094.1645\n",
      "Epoch 396/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 2423.8857 - val_loss: 2527.9482\n",
      "Epoch 397/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 3135.9445 - val_loss: 2931.7764\n",
      "Epoch 398/1000\n",
      "17056/17056 [==============================] - 1s 65us/sample - loss: 3444.7112 - val_loss: 2182.9881\n",
      "Epoch 399/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 2718.0674 - val_loss: 3575.1972\n",
      "Epoch 400/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 3560.0504 - val_loss: 3121.5668\n",
      "Epoch 401/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 2056.6677 - val_loss: 5484.1417\n",
      "Epoch 402/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 3844.0184 - val_loss: 2397.9077\n",
      "Epoch 403/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 2402.9769 - val_loss: 5693.8731\n",
      "Epoch 404/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 2987.0329 - val_loss: 1665.8366\n",
      "Epoch 405/1000\n",
      "17056/17056 [==============================] - 1s 52us/sample - loss: 2642.2292 - val_loss: 6929.2387\n",
      "Epoch 406/1000\n",
      "17056/17056 [==============================] - 1s 51us/sample - loss: 2704.1485 - val_loss: 3750.5923\n",
      "Epoch 407/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 3695.1154 - val_loss: 3832.7429\n",
      "Epoch 408/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 2938.6147 - val_loss: 3400.1676\n",
      "Epoch 409/1000\n",
      "17056/17056 [==============================] - 1s 53us/sample - loss: 1523.7497 - val_loss: 9495.5178\n",
      "Epoch 410/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 4453.8251 - val_loss: 8011.3171\n",
      "Epoch 411/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 2804.8415 - val_loss: 2762.4582\n",
      "Epoch 412/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 1799.0869 - val_loss: 5377.1188\n",
      "Epoch 413/1000\n",
      "17056/17056 [==============================] - 1s 52us/sample - loss: 3761.6502 - val_loss: 20107.2996\n",
      "Epoch 414/1000\n",
      "17056/17056 [==============================] - 1s 52us/sample - loss: 3828.9195 - val_loss: 2435.8301\n",
      "Epoch 415/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 1606.4836 - val_loss: 2029.9851\n",
      "Epoch 416/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 2103.5137 - val_loss: 2519.0867\n",
      "Epoch 417/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 2781.3001 - val_loss: 2367.7526\n",
      "Epoch 418/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 2736.1215 - val_loss: 2862.0774\n",
      "Epoch 419/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 2698.3654 - val_loss: 7881.9623\n",
      "Epoch 420/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17056/17056 [==============================] - 1s 53us/sample - loss: 2183.8413 - val_loss: 1911.3942\n",
      "Epoch 421/1000\n",
      "17056/17056 [==============================] - 1s 52us/sample - loss: 2293.0182 - val_loss: 3645.5808\n",
      "Epoch 422/1000\n",
      "17056/17056 [==============================] - 1s 52us/sample - loss: 3646.5709 - val_loss: 1692.6312\n",
      "Epoch 423/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 2709.6573 - val_loss: 3603.3644\n",
      "Epoch 424/1000\n",
      "17056/17056 [==============================] - 1s 53us/sample - loss: 2776.9532 - val_loss: 2878.4786\n",
      "Epoch 425/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 1767.6884 - val_loss: 2340.3743\n",
      "Epoch 426/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 4132.7672 - val_loss: 5073.4947\n",
      "Epoch 427/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 2201.0514 - val_loss: 3612.1394\n",
      "Epoch 428/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 1903.2093 - val_loss: 2274.1127\n",
      "Epoch 429/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 2831.3274 - val_loss: 3127.6165\n",
      "Epoch 430/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 3377.7201 - val_loss: 15638.2947\n",
      "Epoch 431/1000\n",
      "17056/17056 [==============================] - 1s 53us/sample - loss: 3438.1725 - val_loss: 2783.9438\n",
      "Epoch 432/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 2085.7484 - val_loss: 1963.7923\n",
      "Epoch 433/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 2864.0408 - val_loss: 3428.4430\n",
      "Epoch 434/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 2366.7890 - val_loss: 3952.1450\n",
      "Epoch 435/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 2393.0621 - val_loss: 6381.1972\n",
      "Epoch 436/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 2355.5448 - val_loss: 2033.6214\n",
      "Epoch 437/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 2013.0540 - val_loss: 1823.4084\n",
      "Epoch 438/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 1884.0402 - val_loss: 1664.2969\n",
      "Epoch 439/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 2855.1641 - val_loss: 2448.3832\n",
      "Epoch 440/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 1923.3914 - val_loss: 4053.7576\n",
      "Epoch 441/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 3148.6958 - val_loss: 1901.1778\n",
      "Epoch 442/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 2754.3331 - val_loss: 2221.2163\n",
      "Epoch 443/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 3220.0287 - val_loss: 2943.9570\n",
      "Epoch 444/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 2980.4547 - val_loss: 3290.8683\n",
      "Epoch 445/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 1882.4664 - val_loss: 1871.6087\n",
      "Epoch 446/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 2179.1345 - val_loss: 3158.0589\n",
      "Epoch 447/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 1852.6674 - val_loss: 1651.2998\n",
      "Epoch 448/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 2157.5286 - val_loss: 3822.2653\n",
      "Epoch 449/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 3654.5776 - val_loss: 3773.7924\n",
      "Epoch 450/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 2588.5372 - val_loss: 1878.7278\n",
      "Epoch 451/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 3129.5511 - val_loss: 3084.2510\n",
      "Epoch 452/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 2256.7337 - val_loss: 1490.5260\n",
      "Epoch 453/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 2586.5169 - val_loss: 3067.8366\n",
      "Epoch 454/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 1811.5535 - val_loss: 1564.5032\n",
      "Epoch 455/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 3624.3204 - val_loss: 3078.6838\n",
      "Epoch 456/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 2063.9824 - val_loss: 1881.5397\n",
      "Epoch 457/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 1824.8211 - val_loss: 2081.1431\n",
      "Epoch 458/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 2869.8989 - val_loss: 4396.9327\n",
      "Epoch 459/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 2791.1098 - val_loss: 2709.3993\n",
      "Epoch 460/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 1846.0120 - val_loss: 1779.1623\n",
      "Epoch 461/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 4218.5916 - val_loss: 5069.7394\n",
      "Epoch 462/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 4248.8672 - val_loss: 1962.4225\n",
      "Epoch 463/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 1943.7155 - val_loss: 3809.0365\n",
      "Epoch 464/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 2389.8765 - val_loss: 4650.3086\n",
      "Epoch 465/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 2544.2398 - val_loss: 1604.9730\n",
      "Epoch 466/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 2344.4983 - val_loss: 4735.4785\n",
      "Epoch 467/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 2956.2821 - val_loss: 1981.2671\n",
      "Epoch 468/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 1931.6993 - val_loss: 11366.6176\n",
      "Epoch 469/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 3609.6681 - val_loss: 1750.0227\n",
      "Epoch 470/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 2934.6281 - val_loss: 2205.7177\n",
      "Epoch 471/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 2632.4792 - val_loss: 5627.9290\n",
      "Epoch 472/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 2775.1254 - val_loss: 8350.0750\n",
      "Epoch 473/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 2696.8301 - val_loss: 4790.2025\n",
      "Epoch 474/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 2369.0323 - val_loss: 1675.3723\n",
      "Epoch 475/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 1597.9292 - val_loss: 9530.6278\n",
      "Epoch 476/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 2595.5777 - val_loss: 2321.3331\n",
      "Epoch 477/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 2329.1739 - val_loss: 3966.2411\n",
      "Epoch 478/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 1949.1561 - val_loss: 1956.0887\n",
      "Epoch 479/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 2995.1620 - val_loss: 21309.0847\n",
      "Epoch 480/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 3243.3237 - val_loss: 1872.7692\n",
      "Epoch 481/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 3314.3828 - val_loss: 2944.6609\n",
      "Epoch 482/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 2543.1929 - val_loss: 2197.4063\n",
      "Epoch 483/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 3743.7048 - val_loss: 4043.1594\n",
      "Epoch 484/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 2572.8197 - val_loss: 1479.7053\n",
      "Epoch 485/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 1885.2955 - val_loss: 2928.5174\n",
      "Epoch 486/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 2317.6828 - val_loss: 2169.1399\n",
      "Epoch 487/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 1982.8006 - val_loss: 1718.5592\n",
      "Epoch 488/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 3635.2614 - val_loss: 4710.7263\n",
      "Epoch 489/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 2575.1075 - val_loss: 1972.4124\n",
      "Epoch 490/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17056/17056 [==============================] - 1s 59us/sample - loss: 1364.1111 - val_loss: 1802.7042\n",
      "Epoch 491/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 1929.6612 - val_loss: 5772.9745\n",
      "Epoch 492/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 2804.4813 - val_loss: 3407.5253\n",
      "Epoch 493/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 3866.1297 - val_loss: 1473.4407\n",
      "Epoch 494/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 2101.0696 - val_loss: 1609.3918\n",
      "Epoch 495/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 2618.9486 - val_loss: 2111.3998\n",
      "Epoch 496/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 1716.6630 - val_loss: 2504.6948\n",
      "Epoch 497/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 2716.2418 - val_loss: 2306.3598\n",
      "Epoch 498/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 1631.7504 - val_loss: 2627.4231\n",
      "Epoch 499/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 1845.3469 - val_loss: 1926.2184\n",
      "Epoch 500/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 4380.0420 - val_loss: 3180.1877\n",
      "Epoch 501/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 2346.8371 - val_loss: 2199.4126\n",
      "Epoch 502/1000\n",
      "17056/17056 [==============================] - 1s 65us/sample - loss: 2435.9532 - val_loss: 1819.9497\n",
      "Epoch 503/1000\n",
      "17056/17056 [==============================] - 1s 65us/sample - loss: 1569.8813 - val_loss: 1543.0697\n",
      "Epoch 504/1000\n",
      "17056/17056 [==============================] - 1s 69us/sample - loss: 2783.1282 - val_loss: 4487.9089\n",
      "Epoch 505/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 3018.0772 - val_loss: 1986.1535\n",
      "Epoch 506/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 1841.7483 - val_loss: 3597.1936\n",
      "Epoch 507/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 2249.3568 - val_loss: 9048.1962\n",
      "Epoch 508/1000\n",
      "17056/17056 [==============================] - 1s 67us/sample - loss: 2176.3571 - val_loss: 1815.9031\n",
      "Epoch 509/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 1706.1877 - val_loss: 2134.7635\n",
      "Epoch 510/1000\n",
      "17056/17056 [==============================] - 1s 70us/sample - loss: 2084.6896 - val_loss: 1798.1940\n",
      "Epoch 511/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 2150.9664 - val_loss: 1895.5530\n",
      "Epoch 512/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 1842.9613 - val_loss: 2870.7213\n",
      "Epoch 513/1000\n",
      "17056/17056 [==============================] - 1s 75us/sample - loss: 2594.1973 - val_loss: 4657.1162\n",
      "Epoch 514/1000\n",
      "17056/17056 [==============================] - 1s 71us/sample - loss: 1787.5432 - val_loss: 3156.8073\n",
      "Epoch 515/1000\n",
      "17056/17056 [==============================] - 1s 85us/sample - loss: 5325.9082 - val_loss: 6417.2033\n",
      "Epoch 516/1000\n",
      "17056/17056 [==============================] - 1s 81us/sample - loss: 2237.2702 - val_loss: 6083.3754\n",
      "Epoch 517/1000\n",
      "17056/17056 [==============================] - 1s 72us/sample - loss: 2038.0183 - val_loss: 2979.9300\n",
      "Epoch 518/1000\n",
      "17056/17056 [==============================] - 1s 71us/sample - loss: 1926.6964 - val_loss: 1452.0354\n",
      "Epoch 519/1000\n",
      "17056/17056 [==============================] - 1s 66us/sample - loss: 5190.3487 - val_loss: 3598.8617\n",
      "Epoch 520/1000\n",
      "17056/17056 [==============================] - 1s 67us/sample - loss: 1893.6798 - val_loss: 1761.0711\n",
      "Epoch 521/1000\n",
      "17056/17056 [==============================] - 1s 74us/sample - loss: 1383.6489 - val_loss: 6935.0871\n",
      "Epoch 522/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 2267.0802 - val_loss: 1799.7914\n",
      "Epoch 523/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 1325.5195 - val_loss: 1471.6833\n",
      "Epoch 524/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 2994.2273 - val_loss: 3866.9852\n",
      "Epoch 525/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 3296.9431 - val_loss: 2231.9911\n",
      "Epoch 526/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 2371.0184 - val_loss: 1504.7600\n",
      "Epoch 527/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 1679.3544 - val_loss: 1783.0991\n",
      "Epoch 528/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 2106.8130 - val_loss: 1490.4273\n",
      "Epoch 529/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 1849.2778 - val_loss: 2414.3750\n",
      "Epoch 530/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 2954.7046 - val_loss: 1893.7403\n",
      "Epoch 531/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 2714.4461 - val_loss: 1739.5557\n",
      "Epoch 532/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 2514.3776 - val_loss: 3796.1118\n",
      "Epoch 533/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 2086.4830 - val_loss: 2083.6985\n",
      "Epoch 534/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 1870.7583 - val_loss: 2167.1069\n",
      "Epoch 535/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 2588.0694 - val_loss: 2629.0987\n",
      "Epoch 536/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 1707.0255 - val_loss: 1557.9713\n",
      "Epoch 537/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 2740.1066 - val_loss: 1468.0835\n",
      "Epoch 538/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 3620.7099 - val_loss: 3066.0630\n",
      "Epoch 539/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 1613.8053 - val_loss: 5910.9602\n",
      "Epoch 540/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 3306.5460 - val_loss: 1796.7115\n",
      "Epoch 541/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 1756.2871 - val_loss: 1389.3582\n",
      "Epoch 542/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 1979.9319 - val_loss: 3762.8681\n",
      "Epoch 543/1000\n",
      "17056/17056 [==============================] - 2s 96us/sample - loss: 3163.4584 - val_loss: 3942.2204\n",
      "Epoch 544/1000\n",
      "17056/17056 [==============================] - 1s 87us/sample - loss: 2654.3867 - val_loss: 2539.7564\n",
      "Epoch 545/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 2062.4086 - val_loss: 10702.8513\n",
      "Epoch 546/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 2136.4801 - val_loss: 1899.5490\n",
      "Epoch 547/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 2433.4968 - val_loss: 3570.3072\n",
      "Epoch 548/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 4061.5339 - val_loss: 2046.0395\n",
      "Epoch 549/1000\n",
      "17056/17056 [==============================] - 1s 68us/sample - loss: 1707.2636 - val_loss: 1834.3494\n",
      "Epoch 550/1000\n",
      "17056/17056 [==============================] - 1s 68us/sample - loss: 1785.5663 - val_loss: 1921.0224\n",
      "Epoch 551/1000\n",
      "17056/17056 [==============================] - 1s 69us/sample - loss: 2251.7593 - val_loss: 1158.5504\n",
      "Epoch 552/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 2702.2430 - val_loss: 4624.1780\n",
      "Epoch 553/1000\n",
      "17056/17056 [==============================] - 1s 65us/sample - loss: 2411.1418 - val_loss: 9250.2107\n",
      "Epoch 554/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 2508.7743 - val_loss: 1690.9697\n",
      "Epoch 555/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 1779.2535 - val_loss: 1109.7304\n",
      "Epoch 556/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 1929.3645 - val_loss: 3604.1631\n",
      "Epoch 557/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 3512.7783 - val_loss: 4611.9624\n",
      "Epoch 558/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 1787.6539 - val_loss: 2009.9080\n",
      "Epoch 559/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 2004.2412 - val_loss: 2518.3878\n",
      "Epoch 560/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17056/17056 [==============================] - 1s 59us/sample - loss: 1548.4509 - val_loss: 2035.1507\n",
      "Epoch 561/1000\n",
      "17056/17056 [==============================] - 1s 66us/sample - loss: 2060.0109 - val_loss: 1299.3557\n",
      "Epoch 562/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 2415.6348 - val_loss: 3045.8370\n",
      "Epoch 563/1000\n",
      "17056/17056 [==============================] - 1s 81us/sample - loss: 6236.1167 - val_loss: 3294.1242\n",
      "Epoch 564/1000\n",
      "17056/17056 [==============================] - 3s 149us/sample - loss: 1646.0260 - val_loss: 2279.4542\n",
      "Epoch 565/1000\n",
      "17056/17056 [==============================] - 2s 113us/sample - loss: 1837.7950 - val_loss: 1317.5060\n",
      "Epoch 566/1000\n",
      "17056/17056 [==============================] - 2s 114us/sample - loss: 2397.6807 - val_loss: 4135.7537\n",
      "Epoch 567/1000\n",
      "17056/17056 [==============================] - 1s 71us/sample - loss: 1839.1846 - val_loss: 1881.7717\n",
      "Epoch 568/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 1610.4446 - val_loss: 2215.5209\n",
      "Epoch 569/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 1590.7634 - val_loss: 4284.0389\n",
      "Epoch 570/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 2805.8391 - val_loss: 4462.2478\n",
      "Epoch 571/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 2054.1790 - val_loss: 2872.3165\n",
      "Epoch 572/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 2542.1934 - val_loss: 2400.4766\n",
      "Epoch 573/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 2420.8026 - val_loss: 1581.6454\n",
      "Epoch 574/1000\n",
      "17056/17056 [==============================] - 1s 78us/sample - loss: 2802.5970 - val_loss: 4411.6568\n",
      "Epoch 575/1000\n",
      "17056/17056 [==============================] - 2s 101us/sample - loss: 2476.5760 - val_loss: 3437.4481\n",
      "Epoch 576/1000\n",
      "17056/17056 [==============================] - 2s 100us/sample - loss: 1975.7281 - val_loss: 1547.0287\n",
      "Epoch 577/1000\n",
      "17056/17056 [==============================] - 2s 102us/sample - loss: 2082.4836 - val_loss: 1703.6475\n",
      "Epoch 578/1000\n",
      "17056/17056 [==============================] - 1s 87us/sample - loss: 2081.1145 - val_loss: 1072.6540\n",
      "Epoch 579/1000\n",
      "17056/17056 [==============================] - 2s 93us/sample - loss: 3365.7201 - val_loss: 2132.1696\n",
      "Epoch 580/1000\n",
      "17056/17056 [==============================] - 1s 84us/sample - loss: 1836.1576 - val_loss: 1400.3280\n",
      "Epoch 581/1000\n",
      "17056/17056 [==============================] - 2s 100us/sample - loss: 2791.4146 - val_loss: 1521.5318\n",
      "Epoch 582/1000\n",
      "17056/17056 [==============================] - 1s 81us/sample - loss: 1776.1063 - val_loss: 2861.9922\n",
      "Epoch 583/1000\n",
      "17056/17056 [==============================] - 1s 83us/sample - loss: 2453.5472 - val_loss: 2465.9589\n",
      "Epoch 584/1000\n",
      "17056/17056 [==============================] - 1s 81us/sample - loss: 2478.1480 - val_loss: 1612.2688\n",
      "Epoch 585/1000\n",
      "17056/17056 [==============================] - 2s 88us/sample - loss: 2082.9821 - val_loss: 2052.9524\n",
      "Epoch 586/1000\n",
      "17056/17056 [==============================] - 1s 84us/sample - loss: 1573.4878 - val_loss: 1868.1939\n",
      "Epoch 587/1000\n",
      "17056/17056 [==============================] - 1s 83us/sample - loss: 2024.3580 - val_loss: 1349.4764\n",
      "Epoch 588/1000\n",
      "17056/17056 [==============================] - 1s 81us/sample - loss: 3017.3115 - val_loss: 1932.6191\n",
      "Epoch 589/1000\n",
      "17056/17056 [==============================] - 1s 84us/sample - loss: 1166.3346 - val_loss: 3331.8868\n",
      "Epoch 590/1000\n",
      "17056/17056 [==============================] - 1s 79us/sample - loss: 2107.6410 - val_loss: 2435.8012\n",
      "Epoch 591/1000\n",
      "17056/17056 [==============================] - 1s 82us/sample - loss: 3940.1441 - val_loss: 1507.8099\n",
      "Epoch 592/1000\n",
      "17056/17056 [==============================] - 1s 80us/sample - loss: 1107.6210 - val_loss: 3274.3751\n",
      "Epoch 593/1000\n",
      "17056/17056 [==============================] - 1s 87us/sample - loss: 1882.1330 - val_loss: 3516.1568\n",
      "Epoch 594/1000\n",
      "17056/17056 [==============================] - 1s 80us/sample - loss: 2788.7552 - val_loss: 2921.0292\n",
      "Epoch 595/1000\n",
      "17056/17056 [==============================] - 1s 80us/sample - loss: 2089.9940 - val_loss: 2328.4033\n",
      "Epoch 596/1000\n",
      "17056/17056 [==============================] - 1s 86us/sample - loss: 2518.3553 - val_loss: 3226.5493\n",
      "Epoch 597/1000\n",
      "17056/17056 [==============================] - 1s 84us/sample - loss: 2376.4447 - val_loss: 1195.6301\n",
      "Epoch 598/1000\n",
      "17056/17056 [==============================] - 1s 80us/sample - loss: 1292.0305 - val_loss: 1895.0476\n",
      "Epoch 599/1000\n",
      "17056/17056 [==============================] - 1s 81us/sample - loss: 2255.6721 - val_loss: 1634.5001\n",
      "Epoch 600/1000\n",
      "17056/17056 [==============================] - 1s 83us/sample - loss: 2514.3895 - val_loss: 4109.5256\n",
      "Epoch 601/1000\n",
      "17056/17056 [==============================] - 1s 79us/sample - loss: 1971.0826 - val_loss: 2846.6330\n",
      "Epoch 602/1000\n",
      "17056/17056 [==============================] - 2s 88us/sample - loss: 1605.8793 - val_loss: 5624.3143\n",
      "Epoch 603/1000\n",
      "17056/17056 [==============================] - 2s 88us/sample - loss: 3069.2765 - val_loss: 2484.1180\n",
      "Epoch 604/1000\n",
      "17056/17056 [==============================] - 1s 79us/sample - loss: 2577.2716 - val_loss: 1969.0199\n",
      "Epoch 605/1000\n",
      "17056/17056 [==============================] - 1s 81us/sample - loss: 2479.8308 - val_loss: 1699.3297\n",
      "Epoch 606/1000\n",
      "17056/17056 [==============================] - 2s 88us/sample - loss: 1263.6149 - val_loss: 1583.1194\n",
      "Epoch 607/1000\n",
      "17056/17056 [==============================] - 1s 87us/sample - loss: 1774.7414 - val_loss: 2113.6783\n",
      "Epoch 608/1000\n",
      "17056/17056 [==============================] - 1s 81us/sample - loss: 3213.6042 - val_loss: 1991.6305\n",
      "Epoch 609/1000\n",
      "17056/17056 [==============================] - 1s 80us/sample - loss: 1910.5034 - val_loss: 1103.9327\n",
      "Epoch 610/1000\n",
      "17056/17056 [==============================] - 1s 84us/sample - loss: 1386.0098 - val_loss: 1279.8810\n",
      "Epoch 611/1000\n",
      "17056/17056 [==============================] - 1s 79us/sample - loss: 1456.2744 - val_loss: 1181.3114\n",
      "Epoch 612/1000\n",
      "17056/17056 [==============================] - 1s 82us/sample - loss: 3274.9158 - val_loss: 6263.9173\n",
      "Epoch 613/1000\n",
      "17056/17056 [==============================] - 1s 81us/sample - loss: 2098.5437 - val_loss: 1525.8648\n",
      "Epoch 614/1000\n",
      "17056/17056 [==============================] - 2s 89us/sample - loss: 2221.4012 - val_loss: 1760.5607\n",
      "Epoch 615/1000\n",
      "17056/17056 [==============================] - 1s 85us/sample - loss: 1961.1834 - val_loss: 2851.4823\n",
      "Epoch 616/1000\n",
      "17056/17056 [==============================] - 1s 83us/sample - loss: 1684.5069 - val_loss: 2919.7710\n",
      "Epoch 617/1000\n",
      "17056/17056 [==============================] - 1s 87us/sample - loss: 1980.7111 - val_loss: 2449.2469\n",
      "Epoch 618/1000\n",
      "17056/17056 [==============================] - 1s 86us/sample - loss: 2995.7966 - val_loss: 1465.9735\n",
      "Epoch 619/1000\n",
      "17056/17056 [==============================] - 1s 85us/sample - loss: 1545.4766 - val_loss: 6221.5203\n",
      "Epoch 620/1000\n",
      "17056/17056 [==============================] - 1s 84us/sample - loss: 3663.7200 - val_loss: 2723.3018\n",
      "Epoch 621/1000\n",
      "17056/17056 [==============================] - 2s 91us/sample - loss: 1587.4949 - val_loss: 1569.3925\n",
      "Epoch 622/1000\n",
      "17056/17056 [==============================] - 1s 84us/sample - loss: 1113.7189 - val_loss: 1290.6587\n",
      "Epoch 623/1000\n",
      "17056/17056 [==============================] - 1s 83us/sample - loss: 2118.0128 - val_loss: 1133.2936\n",
      "Epoch 624/1000\n",
      "17056/17056 [==============================] - 1s 87us/sample - loss: 1667.8572 - val_loss: 1830.2554\n",
      "Epoch 625/1000\n",
      "17056/17056 [==============================] - 1s 84us/sample - loss: 1753.2468 - val_loss: 1139.6917\n",
      "Epoch 626/1000\n",
      "17056/17056 [==============================] - 2s 88us/sample - loss: 1752.3301 - val_loss: 1484.2345\n",
      "Epoch 627/1000\n",
      "17056/17056 [==============================] - 1s 87us/sample - loss: 3315.3029 - val_loss: 3968.8257\n",
      "Epoch 628/1000\n",
      "17056/17056 [==============================] - 2s 89us/sample - loss: 2995.8522 - val_loss: 1406.4741\n",
      "Epoch 629/1000\n",
      "17056/17056 [==============================] - 1s 86us/sample - loss: 1289.3312 - val_loss: 4303.9801\n",
      "Epoch 630/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17056/17056 [==============================] - 1s 88us/sample - loss: 1636.4810 - val_loss: 2003.6472\n",
      "Epoch 631/1000\n",
      "17056/17056 [==============================] - 2s 89us/sample - loss: 1493.3602 - val_loss: 1480.9524\n",
      "Epoch 632/1000\n",
      "17056/17056 [==============================] - 2s 94us/sample - loss: 1362.1023 - val_loss: 3998.7314\n",
      "Epoch 633/1000\n",
      "17056/17056 [==============================] - 2s 93us/sample - loss: 2080.9636 - val_loss: 1630.4227\n",
      "Epoch 634/1000\n",
      "17056/17056 [==============================] - 2s 103us/sample - loss: 1023.9330 - val_loss: 984.3841\n",
      "Epoch 635/1000\n",
      "17056/17056 [==============================] - 2s 96us/sample - loss: 3230.0960 - val_loss: 1568.7135\n",
      "Epoch 636/1000\n",
      "17056/17056 [==============================] - 2s 90us/sample - loss: 2156.4058 - val_loss: 2138.2717\n",
      "Epoch 637/1000\n",
      "17056/17056 [==============================] - 2s 97us/sample - loss: 1420.0886 - val_loss: 7845.6723\n",
      "Epoch 638/1000\n",
      "17056/17056 [==============================] - 2s 92us/sample - loss: 2303.9753 - val_loss: 8849.2699\n",
      "Epoch 639/1000\n",
      "17056/17056 [==============================] - 1s 86us/sample - loss: 2335.6145 - val_loss: 1438.1258\n",
      "Epoch 640/1000\n",
      "17056/17056 [==============================] - 2s 89us/sample - loss: 3256.9947 - val_loss: 9459.0059\n",
      "Epoch 641/1000\n",
      "17056/17056 [==============================] - 2s 90us/sample - loss: 2119.3183 - val_loss: 6820.7168\n",
      "Epoch 642/1000\n",
      "17056/17056 [==============================] - 2s 90us/sample - loss: 1556.2230 - val_loss: 2486.5362\n",
      "Epoch 643/1000\n",
      "17056/17056 [==============================] - 2s 101us/sample - loss: 1243.6124 - val_loss: 1361.3989\n",
      "Epoch 644/1000\n",
      "17056/17056 [==============================] - 2s 89us/sample - loss: 1485.8346 - val_loss: 1581.9682\n",
      "Epoch 645/1000\n",
      "17056/17056 [==============================] - 2s 97us/sample - loss: 1789.7224 - val_loss: 1642.4731\n",
      "Epoch 646/1000\n",
      "17056/17056 [==============================] - 2s 110us/sample - loss: 2570.4194 - val_loss: 4830.3420\n",
      "Epoch 647/1000\n",
      "17056/17056 [==============================] - 2s 102us/sample - loss: 1819.9233 - val_loss: 1352.8079\n",
      "Epoch 648/1000\n",
      "17056/17056 [==============================] - 2s 96us/sample - loss: 3536.1511 - val_loss: 831.3767\n",
      "Epoch 649/1000\n",
      "17056/17056 [==============================] - 2s 97us/sample - loss: 1622.0659 - val_loss: 1430.5265\n",
      "Epoch 650/1000\n",
      "17056/17056 [==============================] - 2s 91us/sample - loss: 2576.0093 - val_loss: 3071.0301\n",
      "Epoch 651/1000\n",
      "17056/17056 [==============================] - 2s 90us/sample - loss: 2104.7820 - val_loss: 1611.7267\n",
      "Epoch 652/1000\n",
      "17056/17056 [==============================] - 2s 96us/sample - loss: 1133.8949 - val_loss: 2257.8486\n",
      "Epoch 653/1000\n",
      "17056/17056 [==============================] - 2s 116us/sample - loss: 1852.7603 - val_loss: 2375.6895\n",
      "Epoch 654/1000\n",
      "17056/17056 [==============================] - 2s 112us/sample - loss: 2154.7046 - val_loss: 1328.9347\n",
      "Epoch 655/1000\n",
      "17056/17056 [==============================] - 2s 103us/sample - loss: 1524.6702 - val_loss: 1804.9369\n",
      "Epoch 656/1000\n",
      "17056/17056 [==============================] - 2s 97us/sample - loss: 1387.9880 - val_loss: 1489.7620\n",
      "Epoch 657/1000\n",
      "17056/17056 [==============================] - 2s 91us/sample - loss: 1633.9155 - val_loss: 1376.1990\n",
      "Epoch 658/1000\n",
      "17056/17056 [==============================] - 2s 109us/sample - loss: 1508.3189 - val_loss: 3201.6544\n",
      "Epoch 659/1000\n",
      "17056/17056 [==============================] - 2s 94us/sample - loss: 2547.8638 - val_loss: 3354.9498\n",
      "Epoch 660/1000\n",
      "17056/17056 [==============================] - 2s 95us/sample - loss: 2404.4535 - val_loss: 5914.3560\n",
      "Epoch 661/1000\n",
      "17056/17056 [==============================] - 2s 104us/sample - loss: 2634.0418 - val_loss: 1177.7271\n",
      "Epoch 662/1000\n",
      "17056/17056 [==============================] - 2s 103us/sample - loss: 1813.9910 - val_loss: 5071.8307\n",
      "Epoch 663/1000\n",
      "17056/17056 [==============================] - 3s 181us/sample - loss: 1568.0511 - val_loss: 1423.2264\n",
      "Epoch 664/1000\n",
      "17056/17056 [==============================] - 2s 142us/sample - loss: 1375.2965 - val_loss: 1274.2470\n",
      "Epoch 665/1000\n",
      "17056/17056 [==============================] - 2s 129us/sample - loss: 1717.0111 - val_loss: 2517.6502\n",
      "Epoch 666/1000\n",
      "17056/17056 [==============================] - 3s 161us/sample - loss: 2131.8776 - val_loss: 1914.0653\n",
      "Epoch 667/1000\n",
      "17056/17056 [==============================] - 2s 122us/sample - loss: 3490.6860 - val_loss: 1239.5885\n",
      "Epoch 668/1000\n",
      "17056/17056 [==============================] - 2s 125us/sample - loss: 1391.5195 - val_loss: 886.4540\n",
      "Epoch 669/1000\n",
      "17056/17056 [==============================] - 2s 113us/sample - loss: 1505.3268 - val_loss: 2015.2028\n",
      "Epoch 670/1000\n",
      "17056/17056 [==============================] - 2s 131us/sample - loss: 1524.0391 - val_loss: 1835.7997\n",
      "Epoch 671/1000\n",
      "17056/17056 [==============================] - 2s 127us/sample - loss: 1537.7540 - val_loss: 2239.0140\n",
      "Epoch 672/1000\n",
      "17056/17056 [==============================] - 2s 119us/sample - loss: 2115.9339 - val_loss: 1251.6699\n",
      "Epoch 673/1000\n",
      "17056/17056 [==============================] - 2s 139us/sample - loss: 1954.7425 - val_loss: 2099.6750\n",
      "Epoch 674/1000\n",
      "17056/17056 [==============================] - 2s 128us/sample - loss: 1673.5221 - val_loss: 1897.2518\n",
      "Epoch 675/1000\n",
      "17056/17056 [==============================] - 2s 127us/sample - loss: 1758.2391 - val_loss: 2055.7359\n",
      "Epoch 676/1000\n",
      "17056/17056 [==============================] - 2s 105us/sample - loss: 2852.6438 - val_loss: 1602.7000\n",
      "Epoch 677/1000\n",
      "17056/17056 [==============================] - 2s 103us/sample - loss: 1653.1939 - val_loss: 4064.7001\n",
      "Epoch 678/1000\n",
      "17056/17056 [==============================] - 2s 99us/sample - loss: 1272.8351 - val_loss: 1158.8870\n",
      "Epoch 679/1000\n",
      "17056/17056 [==============================] - 2s 95us/sample - loss: 1962.2349 - val_loss: 4894.7208\n",
      "Epoch 680/1000\n",
      "17056/17056 [==============================] - 2s 93us/sample - loss: 1959.2835 - val_loss: 2886.7407\n",
      "Epoch 681/1000\n",
      "17056/17056 [==============================] - 2s 88us/sample - loss: 2056.5976 - val_loss: 1843.9081\n",
      "Epoch 682/1000\n",
      "17056/17056 [==============================] - 2s 90us/sample - loss: 1274.6427 - val_loss: 3459.5666\n",
      "Epoch 683/1000\n",
      "17056/17056 [==============================] - 2s 88us/sample - loss: 1483.9151 - val_loss: 787.6947\n",
      "Epoch 684/1000\n",
      "17056/17056 [==============================] - 2s 94us/sample - loss: 1662.5464 - val_loss: 1227.7736\n",
      "Epoch 685/1000\n",
      "17056/17056 [==============================] - 2s 92us/sample - loss: 3537.9495 - val_loss: 6003.0355\n",
      "Epoch 686/1000\n",
      "17056/17056 [==============================] - 2s 91us/sample - loss: 2085.6203 - val_loss: 893.4805\n",
      "Epoch 687/1000\n",
      "17056/17056 [==============================] - 2s 93us/sample - loss: 1429.1818 - val_loss: 1638.2254\n",
      "Epoch 688/1000\n",
      "17056/17056 [==============================] - 2s 93us/sample - loss: 2797.8021 - val_loss: 1972.6234\n",
      "Epoch 689/1000\n",
      "17056/17056 [==============================] - 2s 88us/sample - loss: 1324.5330 - val_loss: 1348.3061\n",
      "Epoch 690/1000\n",
      "17056/17056 [==============================] - 2s 93us/sample - loss: 2231.7024 - val_loss: 1112.9379\n",
      "Epoch 691/1000\n",
      "17056/17056 [==============================] - 2s 94us/sample - loss: 1581.5361 - val_loss: 5709.9273\n",
      "Epoch 692/1000\n",
      "17056/17056 [==============================] - 2s 89us/sample - loss: 2744.9591 - val_loss: 1511.5013\n",
      "Epoch 693/1000\n",
      "17056/17056 [==============================] - 2s 92us/sample - loss: 1021.4022 - val_loss: 1325.6810\n",
      "Epoch 694/1000\n",
      "17056/17056 [==============================] - 1s 86us/sample - loss: 954.1982 - val_loss: 1152.9952\n",
      "Epoch 695/1000\n",
      "17056/17056 [==============================] - 2s 90us/sample - loss: 2149.9031 - val_loss: 5317.2115\n",
      "Epoch 696/1000\n",
      "17056/17056 [==============================] - 2s 91us/sample - loss: 2029.4404 - val_loss: 1928.8084\n",
      "Epoch 697/1000\n",
      "17056/17056 [==============================] - 2s 92us/sample - loss: 1583.0483 - val_loss: 1725.8958\n",
      "Epoch 698/1000\n",
      "17056/17056 [==============================] - 2s 90us/sample - loss: 2745.7496 - val_loss: 3242.5376\n",
      "Epoch 699/1000\n",
      "17056/17056 [==============================] - 2s 89us/sample - loss: 3129.6237 - val_loss: 2317.7102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 700/1000\n",
      "17056/17056 [==============================] - 1s 87us/sample - loss: 1896.5201 - val_loss: 981.6078\n",
      "Epoch 701/1000\n",
      "17056/17056 [==============================] - 1s 88us/sample - loss: 1612.2518 - val_loss: 1427.1070\n",
      "Epoch 702/1000\n",
      "17056/17056 [==============================] - 2s 90us/sample - loss: 1585.6213 - val_loss: 2392.0600\n",
      "Epoch 703/1000\n",
      "17056/17056 [==============================] - 2s 94us/sample - loss: 1678.9113 - val_loss: 957.7613\n",
      "Epoch 704/1000\n",
      "17056/17056 [==============================] - 2s 101us/sample - loss: 1777.6675 - val_loss: 4023.3001\n",
      "Epoch 705/1000\n",
      "17056/17056 [==============================] - 2s 100us/sample - loss: 1801.2345 - val_loss: 2302.4941\n",
      "Epoch 706/1000\n",
      "17056/17056 [==============================] - 2s 109us/sample - loss: 1881.5327 - val_loss: 1212.3279\n",
      "Epoch 707/1000\n",
      "17056/17056 [==============================] - 2s 119us/sample - loss: 1333.1274 - val_loss: 1132.3954\n",
      "Epoch 708/1000\n",
      "17056/17056 [==============================] - 2s 121us/sample - loss: 1235.6711 - val_loss: 2193.1978\n",
      "Epoch 709/1000\n",
      "17056/17056 [==============================] - 1s 74us/sample - loss: 2511.8151 - val_loss: 1619.9162\n",
      "Epoch 710/1000\n",
      "17056/17056 [==============================] - 1s 80us/sample - loss: 1841.7069 - val_loss: 1414.0710\n",
      "Epoch 711/1000\n",
      "17056/17056 [==============================] - 1s 68us/sample - loss: 2021.0983 - val_loss: 1905.1932\n",
      "Epoch 712/1000\n",
      "17056/17056 [==============================] - 2s 104us/sample - loss: 1429.1627 - val_loss: 2517.0030\n",
      "Epoch 713/1000\n",
      "17056/17056 [==============================] - 1s 77us/sample - loss: 1869.6029 - val_loss: 7914.1375\n",
      "Epoch 714/1000\n",
      "17056/17056 [==============================] - 1s 69us/sample - loss: 1806.8548 - val_loss: 3661.5697\n",
      "Epoch 715/1000\n",
      "17056/17056 [==============================] - 1s 68us/sample - loss: 2026.1768 - val_loss: 1400.9275\n",
      "Epoch 716/1000\n",
      "17056/17056 [==============================] - 1s 68us/sample - loss: 2244.0996 - val_loss: 1290.3341\n",
      "Epoch 717/1000\n",
      "17056/17056 [==============================] - 1s 69us/sample - loss: 1960.3996 - val_loss: 1584.4732\n",
      "Epoch 718/1000\n",
      "17056/17056 [==============================] - 1s 72us/sample - loss: 1093.6062 - val_loss: 3118.1737\n",
      "Epoch 719/1000\n",
      "17056/17056 [==============================] - 1s 79us/sample - loss: 1972.0631 - val_loss: 1982.4703\n",
      "Epoch 720/1000\n",
      "17056/17056 [==============================] - 2s 98us/sample - loss: 940.0511 - val_loss: 2133.2157\n",
      "Epoch 721/1000\n",
      "17056/17056 [==============================] - 1s 88us/sample - loss: 1613.3530 - val_loss: 3649.6961\n",
      "Epoch 722/1000\n",
      "17056/17056 [==============================] - 1s 67us/sample - loss: 3354.7403 - val_loss: 4244.6962\n",
      "Epoch 723/1000\n",
      "17056/17056 [==============================] - 1s 69us/sample - loss: 2313.6743 - val_loss: 1522.2866\n",
      "Epoch 724/1000\n",
      "17056/17056 [==============================] - 2s 89us/sample - loss: 1309.1470 - val_loss: 1733.2685\n",
      "Epoch 725/1000\n",
      "17056/17056 [==============================] - 1s 66us/sample - loss: 1396.3151 - val_loss: 1937.8193\n",
      "Epoch 726/1000\n",
      "17056/17056 [==============================] - 1s 66us/sample - loss: 1737.3270 - val_loss: 4863.0252\n",
      "Epoch 727/1000\n",
      "17056/17056 [==============================] - 1s 72us/sample - loss: 1347.5830 - val_loss: 1581.1122\n",
      "Epoch 728/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 1611.2429 - val_loss: 2056.2362\n",
      "Epoch 729/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 2088.1382 - val_loss: 2575.3746\n",
      "Epoch 730/1000\n",
      "17056/17056 [==============================] - 1s 55us/sample - loss: 2005.0673 - val_loss: 1531.6026\n",
      "Epoch 731/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 1757.4781 - val_loss: 6978.4569\n",
      "Epoch 732/1000\n",
      "17056/17056 [==============================] - 1s 76us/sample - loss: 1271.6777 - val_loss: 1373.0988\n",
      "Epoch 733/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 864.3070 - val_loss: 1327.4964\n",
      "Epoch 734/1000\n",
      "17056/17056 [==============================] - 1s 54us/sample - loss: 1664.2001 - val_loss: 6732.1266\n",
      "Epoch 735/1000\n",
      "17056/17056 [==============================] - 1s 69us/sample - loss: 1502.5164 - val_loss: 914.2749\n",
      "Epoch 736/1000\n",
      "17056/17056 [==============================] - 1s 74us/sample - loss: 2380.7072 - val_loss: 1729.4292\n",
      "Epoch 737/1000\n",
      "17056/17056 [==============================] - 1s 70us/sample - loss: 3059.2935 - val_loss: 1140.6626\n",
      "Epoch 738/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 1765.5818 - val_loss: 2358.0814\n",
      "Epoch 739/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 1895.1757 - val_loss: 1221.0199\n",
      "Epoch 740/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 1290.2436 - val_loss: 1102.2921\n",
      "Epoch 741/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 1337.5374 - val_loss: 5013.8007\n",
      "Epoch 742/1000\n",
      "17056/17056 [==============================] - 1s 76us/sample - loss: 1729.9255 - val_loss: 7755.9839\n",
      "Epoch 743/1000\n",
      "17056/17056 [==============================] - 1s 66us/sample - loss: 4013.3320 - val_loss: 3404.4500\n",
      "Epoch 744/1000\n",
      "17056/17056 [==============================] - 1s 74us/sample - loss: 1253.3531 - val_loss: 920.7660\n",
      "Epoch 745/1000\n",
      "17056/17056 [==============================] - 1s 85us/sample - loss: 1468.2494 - val_loss: 1936.7916\n",
      "Epoch 746/1000\n",
      "17056/17056 [==============================] - 1s 82us/sample - loss: 1391.8265 - val_loss: 1103.2317\n",
      "Epoch 747/1000\n",
      "17056/17056 [==============================] - 1s 77us/sample - loss: 1261.7015 - val_loss: 2566.6581\n",
      "Epoch 748/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 2360.6729 - val_loss: 1578.8459\n",
      "Epoch 749/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 1525.3229 - val_loss: 3111.3035\n",
      "Epoch 750/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 1948.6686 - val_loss: 1451.2624\n",
      "Epoch 751/1000\n",
      "17056/17056 [==============================] - 1s 76us/sample - loss: 956.3946 - val_loss: 1389.5377\n",
      "Epoch 752/1000\n",
      "17056/17056 [==============================] - 1s 82us/sample - loss: 1174.5366 - val_loss: 1289.0620\n",
      "Epoch 753/1000\n",
      "17056/17056 [==============================] - 2s 118us/sample - loss: 1397.9229 - val_loss: 2473.0233\n",
      "Epoch 754/1000\n",
      "17056/17056 [==============================] - 2s 108us/sample - loss: 1345.4936 - val_loss: 1509.6199\n",
      "Epoch 755/1000\n",
      "17056/17056 [==============================] - 1s 87us/sample - loss: 1182.1288 - val_loss: 3014.5355\n",
      "Epoch 756/1000\n",
      "17056/17056 [==============================] - 1s 70us/sample - loss: 2223.5296 - val_loss: 2712.7888\n",
      "Epoch 757/1000\n",
      "17056/17056 [==============================] - 2s 114us/sample - loss: 2101.9951 - val_loss: 4996.7933\n",
      "Epoch 758/1000\n",
      "17056/17056 [==============================] - 2s 113us/sample - loss: 2574.6751 - val_loss: 970.8887\n",
      "Epoch 759/1000\n",
      "17056/17056 [==============================] - 2s 102us/sample - loss: 1569.6526 - val_loss: 2613.7959\n",
      "Epoch 760/1000\n",
      "17056/17056 [==============================] - 1s 84us/sample - loss: 1637.0549 - val_loss: 954.2301\n",
      "Epoch 761/1000\n",
      "17056/17056 [==============================] - 1s 69us/sample - loss: 1574.5100 - val_loss: 906.0238\n",
      "Epoch 762/1000\n",
      "17056/17056 [==============================] - 2s 103us/sample - loss: 2130.0842 - val_loss: 795.3682\n",
      "Epoch 763/1000\n",
      "17056/17056 [==============================] - 2s 97us/sample - loss: 625.2880 - val_loss: 5159.5343\n",
      "Epoch 764/1000\n",
      "17056/17056 [==============================] - 1s 72us/sample - loss: 1649.9497 - val_loss: 1291.4159\n",
      "Epoch 765/1000\n",
      "17056/17056 [==============================] - 1s 69us/sample - loss: 1239.4488 - val_loss: 1503.0152\n",
      "Epoch 766/1000\n",
      "17056/17056 [==============================] - 1s 71us/sample - loss: 1706.0759 - val_loss: 3423.7051\n",
      "Epoch 767/1000\n",
      "17056/17056 [==============================] - 1s 81us/sample - loss: 1503.2356 - val_loss: 1874.8900\n",
      "Epoch 768/1000\n",
      "17056/17056 [==============================] - 1s 69us/sample - loss: 2868.9903 - val_loss: 1679.8254\n",
      "Epoch 769/1000\n",
      "17056/17056 [==============================] - 1s 84us/sample - loss: 2752.6598 - val_loss: 1576.9093\n",
      "Epoch 770/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17056/17056 [==============================] - 2s 92us/sample - loss: 1232.2183 - val_loss: 1618.4735\n",
      "Epoch 771/1000\n",
      "17056/17056 [==============================] - 2s 91us/sample - loss: 1736.5750 - val_loss: 4248.6798\n",
      "Epoch 772/1000\n",
      "17056/17056 [==============================] - 2s 97us/sample - loss: 1054.3144 - val_loss: 2099.7097\n",
      "Epoch 773/1000\n",
      "17056/17056 [==============================] - 2s 97us/sample - loss: 1718.4814 - val_loss: 2801.6649\n",
      "Epoch 774/1000\n",
      "17056/17056 [==============================] - 1s 86us/sample - loss: 1642.5623 - val_loss: 1406.3915\n",
      "Epoch 775/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 3025.4138 - val_loss: 1654.8161\n",
      "Epoch 776/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 1629.8839 - val_loss: 2329.0876\n",
      "Epoch 777/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 2069.8590 - val_loss: 1073.7974\n",
      "Epoch 778/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 1763.8474 - val_loss: 11149.7417\n",
      "Epoch 779/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 2597.2518 - val_loss: 991.1598\n",
      "Epoch 780/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 891.7523 - val_loss: 1366.0908\n",
      "Epoch 781/1000\n",
      "17056/17056 [==============================] - 1s 71us/sample - loss: 990.0149 - val_loss: 1551.2430\n",
      "Epoch 782/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 1365.8399 - val_loss: 2300.3822\n",
      "Epoch 783/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 1992.6818 - val_loss: 1861.2559\n",
      "Epoch 784/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 1476.4060 - val_loss: 1187.4462\n",
      "Epoch 785/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 1928.5329 - val_loss: 1704.2924\n",
      "Epoch 786/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 826.3402 - val_loss: 853.2506\n",
      "Epoch 787/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 1155.3716 - val_loss: 823.5970\n",
      "Epoch 788/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 1350.7401 - val_loss: 1113.7885\n",
      "Epoch 789/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 2018.9876 - val_loss: 1074.3515\n",
      "Epoch 790/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 2201.3774 - val_loss: 1422.3127\n",
      "Epoch 791/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 1375.5828 - val_loss: 1087.4857\n",
      "Epoch 792/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 2055.0055 - val_loss: 2771.1788\n",
      "Epoch 793/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 2207.5754 - val_loss: 3755.8423\n",
      "Epoch 794/1000\n",
      "17056/17056 [==============================] - 1s 68us/sample - loss: 903.9888 - val_loss: 1156.0333\n",
      "Epoch 795/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 1107.8829 - val_loss: 908.4310\n",
      "Epoch 796/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 1224.4783 - val_loss: 1034.7892\n",
      "Epoch 797/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 2877.1676 - val_loss: 1675.4259\n",
      "Epoch 798/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 1664.7066 - val_loss: 2161.5920\n",
      "Epoch 799/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 1301.8039 - val_loss: 2080.9474\n",
      "Epoch 800/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 1689.4479 - val_loss: 1956.2222\n",
      "Epoch 801/1000\n",
      "17056/17056 [==============================] - 1s 70us/sample - loss: 2043.8989 - val_loss: 3167.9512\n",
      "Epoch 802/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 1306.4877 - val_loss: 1011.0395\n",
      "Epoch 803/1000\n",
      "17056/17056 [==============================] - 1s 67us/sample - loss: 1102.1111 - val_loss: 4286.2882\n",
      "Epoch 804/1000\n",
      "17056/17056 [==============================] - 1s 65us/sample - loss: 2048.2088 - val_loss: 1810.1597\n",
      "Epoch 805/1000\n",
      "17056/17056 [==============================] - 1s 67us/sample - loss: 1178.7778 - val_loss: 1346.5293\n",
      "Epoch 806/1000\n",
      "17056/17056 [==============================] - 1s 65us/sample - loss: 1595.2376 - val_loss: 926.1960\n",
      "Epoch 807/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 1447.0247 - val_loss: 4158.8133\n",
      "Epoch 808/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 2223.3524 - val_loss: 4096.7411\n",
      "Epoch 809/1000\n",
      "17056/17056 [==============================] - 1s 65us/sample - loss: 3239.2956 - val_loss: 1079.1206\n",
      "Epoch 810/1000\n",
      "17056/17056 [==============================] - 1s 68us/sample - loss: 1386.0675 - val_loss: 1330.9795\n",
      "Epoch 811/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 1074.5767 - val_loss: 1618.4569\n",
      "Epoch 812/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 1510.9440 - val_loss: 766.5429\n",
      "Epoch 813/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 1571.6699 - val_loss: 2241.6866\n",
      "Epoch 814/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 1759.2438 - val_loss: 1873.3339\n",
      "Epoch 815/1000\n",
      "17056/17056 [==============================] - 1s 67us/sample - loss: 1624.1621 - val_loss: 1029.9639\n",
      "Epoch 816/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 1005.1505 - val_loss: 1007.7339\n",
      "Epoch 817/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 1665.1794 - val_loss: 1386.9138\n",
      "Epoch 818/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 1463.7056 - val_loss: 2402.7913\n",
      "Epoch 819/1000\n",
      "17056/17056 [==============================] - 1s 65us/sample - loss: 1795.0637 - val_loss: 1014.4101\n",
      "Epoch 820/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 1058.6148 - val_loss: 1892.9136\n",
      "Epoch 821/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 1781.0397 - val_loss: 3591.5946\n",
      "Epoch 822/1000\n",
      "17056/17056 [==============================] - 1s 65us/sample - loss: 1069.3988 - val_loss: 2172.6090\n",
      "Epoch 823/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 1691.3264 - val_loss: 1662.8463\n",
      "Epoch 824/1000\n",
      "17056/17056 [==============================] - 1s 66us/sample - loss: 1428.2817 - val_loss: 2360.0107\n",
      "Epoch 825/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 2666.5344 - val_loss: 1906.7345\n",
      "Epoch 826/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 1967.0948 - val_loss: 1067.4288\n",
      "Epoch 827/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 1237.5937 - val_loss: 1871.3050\n",
      "Epoch 828/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 1581.2760 - val_loss: 1424.6554\n",
      "Epoch 829/1000\n",
      "17056/17056 [==============================] - 1s 66us/sample - loss: 918.0264 - val_loss: 963.3675\n",
      "Epoch 830/1000\n",
      "17056/17056 [==============================] - 1s 65us/sample - loss: 825.1065 - val_loss: 1502.5578\n",
      "Epoch 831/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 1583.5265 - val_loss: 1231.9234\n",
      "Epoch 832/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 1547.5779 - val_loss: 1991.0837\n",
      "Epoch 833/1000\n",
      "17056/17056 [==============================] - 1s 66us/sample - loss: 1679.1546 - val_loss: 2354.4146\n",
      "Epoch 834/1000\n",
      "17056/17056 [==============================] - 1s 72us/sample - loss: 2662.1315 - val_loss: 1845.6290\n",
      "Epoch 835/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 1642.6263 - val_loss: 1629.6206\n",
      "Epoch 836/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 757.5896 - val_loss: 1587.8940\n",
      "Epoch 837/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 2004.2496 - val_loss: 970.1866\n",
      "Epoch 838/1000\n",
      "17056/17056 [==============================] - 1s 66us/sample - loss: 696.3091 - val_loss: 716.8802\n",
      "Epoch 839/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 3600.2666 - val_loss: 1697.6732\n",
      "Epoch 840/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17056/17056 [==============================] - 1s 63us/sample - loss: 1202.7239 - val_loss: 1215.5029\n",
      "Epoch 841/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 1373.6626 - val_loss: 819.2942\n",
      "Epoch 842/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 1867.3134 - val_loss: 1629.3788\n",
      "Epoch 843/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 1704.6581 - val_loss: 1869.5036\n",
      "Epoch 844/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 1233.7557 - val_loss: 1385.1090\n",
      "Epoch 845/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 1039.1844 - val_loss: 5066.3369\n",
      "Epoch 846/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 1719.0817 - val_loss: 3018.1877\n",
      "Epoch 847/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 1359.7659 - val_loss: 2141.7114\n",
      "Epoch 848/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 1689.1302 - val_loss: 1539.3236\n",
      "Epoch 849/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 1306.0593 - val_loss: 1447.1652\n",
      "Epoch 850/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 1053.4412 - val_loss: 4628.7542\n",
      "Epoch 851/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 1891.2842 - val_loss: 1924.3688\n",
      "Epoch 852/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 1383.9270 - val_loss: 1096.9719\n",
      "Epoch 853/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 1229.8684 - val_loss: 939.1500\n",
      "Epoch 854/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 2169.6728 - val_loss: 8672.7199\n",
      "Epoch 855/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 2942.3828 - val_loss: 1981.7605\n",
      "Epoch 856/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 1559.4139 - val_loss: 1534.1473\n",
      "Epoch 857/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 2104.3757 - val_loss: 868.2974\n",
      "Epoch 858/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 1011.5208 - val_loss: 4691.7176\n",
      "Epoch 859/1000\n",
      "17056/17056 [==============================] - 1s 68us/sample - loss: 1584.1309 - val_loss: 1281.1526\n",
      "Epoch 860/1000\n",
      "17056/17056 [==============================] - 1s 70us/sample - loss: 1488.2810 - val_loss: 1907.6001\n",
      "Epoch 861/1000\n",
      "17056/17056 [==============================] - 1s 74us/sample - loss: 1242.6185 - val_loss: 1551.4098\n",
      "Epoch 862/1000\n",
      "17056/17056 [==============================] - 2s 105us/sample - loss: 1781.5883 - val_loss: 4680.5170\n",
      "Epoch 863/1000\n",
      "17056/17056 [==============================] - 2s 115us/sample - loss: 1104.7608 - val_loss: 1742.9039\n",
      "Epoch 864/1000\n",
      "17056/17056 [==============================] - 2s 108us/sample - loss: 843.2889 - val_loss: 1742.6552\n",
      "Epoch 865/1000\n",
      "17056/17056 [==============================] - 2s 115us/sample - loss: 2638.6312 - val_loss: 1051.9296\n",
      "Epoch 866/1000\n",
      "17056/17056 [==============================] - 2s 116us/sample - loss: 1318.2704 - val_loss: 2851.2451\n",
      "Epoch 867/1000\n",
      "17056/17056 [==============================] - 2s 125us/sample - loss: 2219.8337 - val_loss: 1530.5352\n",
      "Epoch 868/1000\n",
      "17056/17056 [==============================] - 2s 102us/sample - loss: 876.8032 - val_loss: 1534.0721\n",
      "Epoch 869/1000\n",
      "17056/17056 [==============================] - 2s 110us/sample - loss: 1511.2929 - val_loss: 3773.6926\n",
      "Epoch 870/1000\n",
      "17056/17056 [==============================] - 2s 104us/sample - loss: 1941.4060 - val_loss: 3260.7599\n",
      "Epoch 871/1000\n",
      "17056/17056 [==============================] - 2s 139us/sample - loss: 1803.1975 - val_loss: 914.4610\n",
      "Epoch 872/1000\n",
      "17056/17056 [==============================] - 1s 84us/sample - loss: 1428.3666 - val_loss: 4947.8924\n",
      "Epoch 873/1000\n",
      "17056/17056 [==============================] - 1s 69us/sample - loss: 1741.3829 - val_loss: 2113.1071\n",
      "Epoch 874/1000\n",
      "17056/17056 [==============================] - 1s 71us/sample - loss: 932.8675 - val_loss: 1048.5818\n",
      "Epoch 875/1000\n",
      "17056/17056 [==============================] - 1s 77us/sample - loss: 858.6821 - val_loss: 835.9985\n",
      "Epoch 876/1000\n",
      "17056/17056 [==============================] - 1s 76us/sample - loss: 1197.7252 - val_loss: 2806.7453\n",
      "Epoch 877/1000\n",
      "17056/17056 [==============================] - 1s 65us/sample - loss: 1880.0029 - val_loss: 3600.7848\n",
      "Epoch 878/1000\n",
      "17056/17056 [==============================] - 1s 67us/sample - loss: 3254.2613 - val_loss: 3033.5116\n",
      "Epoch 879/1000\n",
      "17056/17056 [==============================] - 1s 86us/sample - loss: 1067.9083 - val_loss: 2533.4591\n",
      "Epoch 880/1000\n",
      "17056/17056 [==============================] - 1s 88us/sample - loss: 935.9085 - val_loss: 1225.3181\n",
      "Epoch 881/1000\n",
      "17056/17056 [==============================] - 1s 87us/sample - loss: 1668.0575 - val_loss: 1219.7391\n",
      "Epoch 882/1000\n",
      "17056/17056 [==============================] - 1s 72us/sample - loss: 1123.9359 - val_loss: 1271.1813\n",
      "Epoch 883/1000\n",
      "17056/17056 [==============================] - 1s 82us/sample - loss: 1733.5209 - val_loss: 2592.0537\n",
      "Epoch 884/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 1655.2561 - val_loss: 1318.4398\n",
      "Epoch 885/1000\n",
      "17056/17056 [==============================] - 1s 77us/sample - loss: 926.4398 - val_loss: 4100.7606\n",
      "Epoch 886/1000\n",
      "17056/17056 [==============================] - 1s 66us/sample - loss: 1321.4448 - val_loss: 1512.2495\n",
      "Epoch 887/1000\n",
      "17056/17056 [==============================] - 1s 86us/sample - loss: 1557.9842 - val_loss: 7759.4823\n",
      "Epoch 888/1000\n",
      "17056/17056 [==============================] - 1s 66us/sample - loss: 2213.5806 - val_loss: 1859.9480\n",
      "Epoch 889/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 1113.4272 - val_loss: 838.1267\n",
      "Epoch 890/1000\n",
      "17056/17056 [==============================] - 1s 83us/sample - loss: 1713.1406 - val_loss: 2647.9695\n",
      "Epoch 891/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 1195.8059 - val_loss: 1719.4184\n",
      "Epoch 892/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 1067.5843 - val_loss: 1068.5773\n",
      "Epoch 893/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 1358.7242 - val_loss: 1974.3750\n",
      "Epoch 894/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 2172.0232 - val_loss: 1048.3639\n",
      "Epoch 895/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 1007.1033 - val_loss: 1623.3473\n",
      "Epoch 896/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 1893.3226 - val_loss: 1432.7140\n",
      "Epoch 897/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 1735.6648 - val_loss: 1994.7863\n",
      "Epoch 898/1000\n",
      "17056/17056 [==============================] - 1s 56us/sample - loss: 775.3803 - val_loss: 3498.3619\n",
      "Epoch 899/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 1267.3737 - val_loss: 2715.1198\n",
      "Epoch 900/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 1178.7458 - val_loss: 799.5172\n",
      "Epoch 901/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 1937.0487 - val_loss: 2898.5274\n",
      "Epoch 902/1000\n",
      "17056/17056 [==============================] - 1s 71us/sample - loss: 1044.9174 - val_loss: 1080.0443\n",
      "Epoch 903/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 1350.6795 - val_loss: 3462.8687\n",
      "Epoch 904/1000\n",
      "17056/17056 [==============================] - 1s 84us/sample - loss: 2853.5416 - val_loss: 1319.5101\n",
      "Epoch 905/1000\n",
      "17056/17056 [==============================] - 1s 86us/sample - loss: 1143.2388 - val_loss: 726.3133\n",
      "Epoch 906/1000\n",
      "17056/17056 [==============================] - 1s 69us/sample - loss: 1449.1867 - val_loss: 839.9412\n",
      "Epoch 907/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 1421.0342 - val_loss: 1517.7235\n",
      "Epoch 908/1000\n",
      "17056/17056 [==============================] - 1s 67us/sample - loss: 1152.3304 - val_loss: 2194.1814\n",
      "Epoch 909/1000\n",
      "17056/17056 [==============================] - 1s 69us/sample - loss: 1459.0624 - val_loss: 3402.5300\n",
      "Epoch 910/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17056/17056 [==============================] - 1s 75us/sample - loss: 1505.1125 - val_loss: 4192.5812\n",
      "Epoch 911/1000\n",
      "17056/17056 [==============================] - 1s 70us/sample - loss: 1582.3527 - val_loss: 1715.0823\n",
      "Epoch 912/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 1006.3509 - val_loss: 1496.5577\n",
      "Epoch 913/1000\n",
      "17056/17056 [==============================] - 1s 67us/sample - loss: 1847.3005 - val_loss: 1945.8657\n",
      "Epoch 914/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 1105.6185 - val_loss: 2879.4945\n",
      "Epoch 915/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 2076.9169 - val_loss: 1388.9617\n",
      "Epoch 916/1000\n",
      "17056/17056 [==============================] - 1s 68us/sample - loss: 1607.6700 - val_loss: 2527.3778\n",
      "Epoch 917/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 1207.8517 - val_loss: 1035.8917\n",
      "Epoch 918/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 2080.0291 - val_loss: 9064.7714\n",
      "Epoch 919/1000\n",
      "17056/17056 [==============================] - 1s 71us/sample - loss: 1936.5835 - val_loss: 2847.8968\n",
      "Epoch 920/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 1363.3122 - val_loss: 2296.4728\n",
      "Epoch 921/1000\n",
      "17056/17056 [==============================] - 1s 65us/sample - loss: 1185.6137 - val_loss: 1707.5454\n",
      "Epoch 922/1000\n",
      "17056/17056 [==============================] - 1s 69us/sample - loss: 745.3079 - val_loss: 2489.2649\n",
      "Epoch 923/1000\n",
      "17056/17056 [==============================] - 1s 68us/sample - loss: 1922.9446 - val_loss: 2561.0153\n",
      "Epoch 924/1000\n",
      "17056/17056 [==============================] - 1s 72us/sample - loss: 952.9651 - val_loss: 963.5023\n",
      "Epoch 925/1000\n",
      "17056/17056 [==============================] - 1s 65us/sample - loss: 1506.4954 - val_loss: 3331.8134\n",
      "Epoch 926/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 691.7952 - val_loss: 1155.7850\n",
      "Epoch 927/1000\n",
      "17056/17056 [==============================] - 1s 68us/sample - loss: 1556.3393 - val_loss: 2899.9392\n",
      "Epoch 928/1000\n",
      "17056/17056 [==============================] - 1s 72us/sample - loss: 1982.8496 - val_loss: 1111.9047\n",
      "Epoch 929/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 1162.3619 - val_loss: 1269.6722\n",
      "Epoch 930/1000\n",
      "17056/17056 [==============================] - 1s 68us/sample - loss: 1466.6715 - val_loss: 1185.2721\n",
      "Epoch 931/1000\n",
      "17056/17056 [==============================] - 1s 68us/sample - loss: 1409.9626 - val_loss: 1279.2916\n",
      "Epoch 932/1000\n",
      "17056/17056 [==============================] - 1s 74us/sample - loss: 1786.5113 - val_loss: 3546.0142\n",
      "Epoch 933/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 2529.7372 - val_loss: 1326.1470\n",
      "Epoch 934/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 1074.8813 - val_loss: 940.2388\n",
      "Epoch 935/1000\n",
      "17056/17056 [==============================] - 1s 67us/sample - loss: 828.5604 - val_loss: 1992.2669\n",
      "Epoch 936/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 1605.8946 - val_loss: 2267.4480\n",
      "Epoch 937/1000\n",
      "17056/17056 [==============================] - 1s 67us/sample - loss: 2326.2669 - val_loss: 4085.5052\n",
      "Epoch 938/1000\n",
      "17056/17056 [==============================] - 1s 70us/sample - loss: 1404.4634 - val_loss: 1934.7160\n",
      "Epoch 939/1000\n",
      "17056/17056 [==============================] - 1s 75us/sample - loss: 948.6017 - val_loss: 870.3436\n",
      "Epoch 940/1000\n",
      "17056/17056 [==============================] - 1s 77us/sample - loss: 1235.8843 - val_loss: 2399.2305\n",
      "Epoch 941/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 1707.7673 - val_loss: 1722.9182\n",
      "Epoch 942/1000\n",
      "17056/17056 [==============================] - 1s 69us/sample - loss: 1111.3387 - val_loss: 1244.6523\n",
      "Epoch 943/1000\n",
      "17056/17056 [==============================] - 1s 69us/sample - loss: 1411.6047 - val_loss: 1164.7188\n",
      "Epoch 944/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 1303.0155 - val_loss: 1017.4612\n",
      "Epoch 945/1000\n",
      "17056/17056 [==============================] - 1s 70us/sample - loss: 2456.5794 - val_loss: 3186.8758\n",
      "Epoch 946/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 1705.6573 - val_loss: 2210.3117\n",
      "Epoch 947/1000\n",
      "17056/17056 [==============================] - 1s 66us/sample - loss: 2212.6246 - val_loss: 3492.9461\n",
      "Epoch 948/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 1907.0298 - val_loss: 2776.3179\n",
      "Epoch 949/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 1347.5171 - val_loss: 1228.6849\n",
      "Epoch 950/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 1671.7410 - val_loss: 939.0197\n",
      "Epoch 951/1000\n",
      "17056/17056 [==============================] - 1s 75us/sample - loss: 1260.2248 - val_loss: 1175.4966\n",
      "Epoch 952/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 883.8210 - val_loss: 2241.2502\n",
      "Epoch 953/1000\n",
      "17056/17056 [==============================] - 1s 69us/sample - loss: 1025.0815 - val_loss: 4535.8614\n",
      "Epoch 954/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 2413.8101 - val_loss: 795.2780\n",
      "Epoch 955/1000\n",
      "17056/17056 [==============================] - 1s 71us/sample - loss: 1443.2950 - val_loss: 1205.3625\n",
      "Epoch 956/1000\n",
      "17056/17056 [==============================] - 1s 67us/sample - loss: 2147.9302 - val_loss: 1579.1874\n",
      "Epoch 957/1000\n",
      "17056/17056 [==============================] - 1s 70us/sample - loss: 1105.3117 - val_loss: 2920.3832\n",
      "Epoch 958/1000\n",
      "17056/17056 [==============================] - 1s 70us/sample - loss: 1763.0883 - val_loss: 661.3802\n",
      "Epoch 959/1000\n",
      "17056/17056 [==============================] - 1s 75us/sample - loss: 1015.3081 - val_loss: 1079.2649\n",
      "Epoch 960/1000\n",
      "17056/17056 [==============================] - 1s 66us/sample - loss: 916.8959 - val_loss: 4432.8529\n",
      "Epoch 961/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 943.0721 - val_loss: 801.9901\n",
      "Epoch 962/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 1272.2291 - val_loss: 1577.4829\n",
      "Epoch 963/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 1712.1258 - val_loss: 1554.1815\n",
      "Epoch 964/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 2099.1970 - val_loss: 2241.9025\n",
      "Epoch 965/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 2015.6775 - val_loss: 1204.4155\n",
      "Epoch 966/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 1011.7576 - val_loss: 834.5033\n",
      "Epoch 967/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 2243.2587 - val_loss: 1570.6992\n",
      "Epoch 968/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 1484.2824 - val_loss: 2661.4092\n",
      "Epoch 969/1000\n",
      "17056/17056 [==============================] - 1s 66us/sample - loss: 1919.5083 - val_loss: 1266.1426\n",
      "Epoch 970/1000\n",
      "17056/17056 [==============================] - 1s 68us/sample - loss: 1590.8306 - val_loss: 1840.0820\n",
      "Epoch 971/1000\n",
      "17056/17056 [==============================] - 1s 70us/sample - loss: 1287.1073 - val_loss: 2551.1427\n",
      "Epoch 972/1000\n",
      "17056/17056 [==============================] - 1s 72us/sample - loss: 944.6393 - val_loss: 3172.3013\n",
      "Epoch 973/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 1910.6671 - val_loss: 2357.9087\n",
      "Epoch 974/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 1179.4159 - val_loss: 2628.5286\n",
      "Epoch 975/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 744.9937 - val_loss: 1349.5373\n",
      "Epoch 976/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 692.8929 - val_loss: 1318.6707\n",
      "Epoch 977/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 1030.6721 - val_loss: 1877.9719\n",
      "Epoch 978/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 1119.1187 - val_loss: 1597.2810\n",
      "Epoch 979/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 1476.7774 - val_loss: 2308.0509\n",
      "Epoch 980/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17056/17056 [==============================] - 1s 60us/sample - loss: 1545.8722 - val_loss: 2335.4874\n",
      "Epoch 981/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 1607.2104 - val_loss: 2738.6251\n",
      "Epoch 982/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 2400.2216 - val_loss: 5779.8207\n",
      "Epoch 983/1000\n",
      "17056/17056 [==============================] - 1s 61us/sample - loss: 2423.0094 - val_loss: 3907.9853\n",
      "Epoch 984/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 1296.1050 - val_loss: 1020.4962\n",
      "Epoch 985/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 1785.5224 - val_loss: 1902.0481\n",
      "Epoch 986/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 974.9282 - val_loss: 821.4984\n",
      "Epoch 987/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 1259.7353 - val_loss: 971.6140\n",
      "Epoch 988/1000\n",
      "17056/17056 [==============================] - 1s 60us/sample - loss: 895.5456 - val_loss: 842.4097\n",
      "Epoch 989/1000\n",
      "17056/17056 [==============================] - 1s 58us/sample - loss: 2384.2256 - val_loss: 1856.5155\n",
      "Epoch 990/1000\n",
      "17056/17056 [==============================] - 1s 57us/sample - loss: 1149.8113 - val_loss: 1771.3407\n",
      "Epoch 991/1000\n",
      "17056/17056 [==============================] - 1s 63us/sample - loss: 810.5760 - val_loss: 1139.9662\n",
      "Epoch 992/1000\n",
      "17056/17056 [==============================] - 1s 62us/sample - loss: 1218.6183 - val_loss: 944.8454\n",
      "Epoch 993/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 893.6000 - val_loss: 1876.9250\n",
      "Epoch 994/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 1825.4721 - val_loss: 1228.5437\n",
      "Epoch 995/1000\n",
      "17056/17056 [==============================] - 1s 59us/sample - loss: 867.5312 - val_loss: 767.9262\n",
      "Epoch 996/1000\n",
      "17056/17056 [==============================] - 1s 64us/sample - loss: 2322.7066 - val_loss: 1995.1152\n",
      "Epoch 997/1000\n",
      "17056/17056 [==============================] - 1s 72us/sample - loss: 2477.2990 - val_loss: 2040.7557\n",
      "Epoch 998/1000\n",
      "17056/17056 [==============================] - 1s 69us/sample - loss: 1518.0080 - val_loss: 973.8226\n",
      "Epoch 999/1000\n",
      "17056/17056 [==============================] - 1s 67us/sample - loss: 1444.3688 - val_loss: 2926.5052\n",
      "Epoch 1000/1000\n",
      "17056/17056 [==============================] - 1s 69us/sample - loss: 1589.6567 - val_loss: 1389.4011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x207c1291c88>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x207c58886c8>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.78075 ],\n",
       "       [-30.769932],\n",
       "       [ -1.302534],\n",
       "       ...,\n",
       "       [-50.627415],\n",
       "       [ -7.145893],\n",
       "       [ 14.648792]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x207c7b38c48>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(y_test,prediction[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
